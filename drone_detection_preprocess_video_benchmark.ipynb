{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drone detection video benchmark (YOLO) — baseline vs adaptive ROI (v4)\n",
    "\n",
    "This notebook benchmarks drone detection on a video while drawing overlays on **every video frame** and running YOLO at a configurable inference rate (e.g., **5 FPS**).\n",
    "\n",
    "It is designed for your exact workflow:\n",
    "\n",
    "- **Baseline** exists and is selected by:\n",
    "  - `PREPROCESS_MODE = \"None\"` and `VERIFY_MODE = \"None\"`\n",
    "- **Adaptive ROI** features are selectable and (mostly) stackable:\n",
    "  - Guided ROI (crop around previous detected drone)\n",
    "  - Motion-based ROIs (1–3 moving-object crops after a no-drone streak)\n",
    "  - Verification passes using the **same YOLO weights**:\n",
    "    - `\"DoublePassHighConf\"` / `\"DoublePassHighRes\"` / `\"DoublePassTiled\"`\n",
    "  - Tiny-object rescue mode (tiled inference when the object is tiny)\n",
    "\n",
    "Hard rules enforced:\n",
    "\n",
    "- **4-out-of-5 confirmation** is always enabled (common to all modes).\n",
    "- **Continuity gating** is selectable (3 methods) and used inside confirmation windows.\n",
    "- **One inference step = one hit** no matter how many ROIs are inferred in that step.\n",
    "- **mAP50** is computed on **inference frames only** (and label is always exactly `drone` in annotation).\n",
    "\n",
    "Outputs are written to `video_benchmark_outputs/` in your project root.\n",
    "\n",
    "Last generated: 2026-01-23 10:10:35\n"
   ],
   "id": "f60818a7e37e25f3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What the main building blocks mean\n",
    "\n",
    "### 1) Preprocessing (selects what image YOLO sees)\n",
    "- **None**: YOLO runs on the full frame (baseline).\n",
    "- **Guided ROI**: if YOLO found a drone previously, crop a 640×640 region centered on that detection and run YOLO on it.\n",
    "- **Motion ROIs**: after a no-drone streak, find up to 3 moving regions and run YOLO on each crop (still counts as 1 hit for the step).\n",
    "\n",
    "### 2) Verification (optional extra pass using the SAME YOLO weights)\n",
    "This is **not** the 4/5 confirmation. This is a second YOLO pass used to suppress false positives.\n",
    "\n",
    "`VERIFY_MODE` options:\n",
    "- **None**: no extra pass (baseline when combined with PREPROCESS_MODE=None).\n",
    "- **DoublePassHighConf**: re-run YOLO on a crop around the detected box using a higher confidence threshold.\n",
    "- **DoublePassHighRes**: re-run YOLO on a crop around the detected box using a larger `imgsz` (more detail, slower).\n",
    "- **DoublePassTiled**: re-run YOLO on tiles (slicing) to help tiny objects.\n",
    "\n",
    "### 3) Confirmation (always-on, common to all)\n",
    "A detection becomes a confirmed event if:\n",
    "- it is a hit in **≥4 of the last 5 inference steps**, AND\n",
    "- continuity gating passes (selectable method)\n",
    "\n",
    "We also track a separate **warning window** that includes small detections (for later filtering).\n"
   ],
   "id": "cdb8a1e69d042db1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What each confidence threshold does (practical)\n",
    "\n",
    "- `BASE_CONF_FULL`: minimum confidence for detections from full-frame YOLO and motion ROI YOLO.\n",
    "  - Lower → higher recall, more false positives (birds).\n",
    "- `GUIDED_ROI_CONF`: minimum confidence when using guided ROI (previous detection crop).\n",
    "  - Often set higher because search area is smaller.\n",
    "- `LOW_CONF_DETECTION`: if the best detection confidence is below this, the detection is considered **suspect**.\n",
    "  - Suspect detections are forced into a verification pass on the **same inference step** (confirmation guided ROI).\n",
    "\n",
    "Two important flags:\n",
    "- `ENABLE_CONFIRMATION_GUIDED_ROI_ALL`:\n",
    "  - If True, **every** detection must be verified by a secondary pass before it can be counted.\n",
    "  - Default False.\n",
    "- `FORCE_VERIFY_SMALL_OR_LOWCONF`:\n",
    "  - Always True unless you are in baseline.\n",
    "  - Forces verification when the detection is tiny or low confidence.\n"
   ],
   "id": "958a391f3dfeb3ac"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many frames does motion detection need to buffer?\n",
    "\n",
    "If the camera is mostly static (your case), **3–5 inference frames** is typically enough.\n",
    "- More frames (7–9) can reduce noise, but increases lag and can smear motion.\n",
    "- The more important knob for leaves/trees is **temporal consistency**: we only keep motion pixels that appear in **multiple** of the buffer differences.\n",
    "\n",
    "This notebook defaults to 5 buffered inference frames and requires motion to appear in at least 3 of the 4 differences (tunable).\n"
   ],
   "id": "aadd1849a055f683"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cell 1 — Imports\n",
    "import os, time, math, json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ultralytics import YOLO\n"
   ],
   "id": "193b5a7fc24aa16c"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cell 2 — Config (your real paths + benchmark options)\n",
    "\n",
    "# --- Your real paths (keep these exactly as you use them) ---\n",
    "MODEL_WEIGHTS = r\"S:\\IntelliJ\\Projects\\ES_Drone_Detection\\runs\\detect\\yolo26\\drone_finetune_full_mixed\\weights\\best.pt\"\n",
    "VIDEO_PATH    = r\"S:\\IntelliJ\\Projects\\ES_Drone_Detection\\video_test\\gopro_006.mp4\"\n",
    "\n",
    "# Annotation path: prefer your local file; else use the uploaded example (GOPR5844_002.txt).\n",
    "ANNOTATION_PATH_PRIMARY  = r\"S:\\IntelliJ\\Projects\\ES_Drone_Detection\\video_test\\gopro_006.txt\"\n",
    "ANNOTATION_PATH_FALLBACK = r\"/mnt/data/gopro_006.txt\"\n",
    "ANNOTATION_PATH = ANNOTATION_PATH_PRIMARY if os.path.exists(ANNOTATION_PATH_PRIMARY) else ANNOTATION_PATH_FALLBACK\n",
    "\n",
    "# --- Output root folder (MUST be project-root/video_benchmark_outputs/) ---\n",
    "OUTPUT_ROOT = \"video_benchmark_outputs\"\n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "\n",
    "# --- Inference policy ---\n",
    "INFER_FPS = 5          # inference steps per second (e.g., 5)\n",
    "ROI_SIZE = 640         # crop size fed to YOLO\n",
    "MAX_FULLFRAME_DETECTIONS = 3\n",
    "MAX_ROI_DETECTIONS = 1\n",
    "\n",
    "# --- Core mode selection ---\n",
    "# Baseline = PREPROCESS_MODE=\"None\" and VERIFY_MODE=\"None\"\n",
    "PREPROCESS_MODE = \"Auto\"  # \"None\" | \"Auto\"\n",
    "USE_GUIDED_ROI = True     # stackable\n",
    "USE_MOTION_ROIS = True    # stackable\n",
    "\n",
    "# Motion ROI method (choose ONE — not stackable)\n",
    "MOTION_METHOD = \"ORB_Affine_StabilizedDiff\"  # see Cell 5 for options\n",
    "\n",
    "# Continuity gate (choose ONE)\n",
    "CONTINUITY_GATE_MODE = \"ExpandedIoU\"  # \"ExpandedIoU\" | \"CenterDistance\" | \"Kalman\"\n",
    "\n",
    "# Verification pass using SAME YOLO weights (choose ONE)\n",
    "VERIFY_MODE = \"None\"  # \"None\" | \"DoublePassHighConf\" | \"DoublePassHighRes\" | \"DoublePassTiled\"\n",
    "\n",
    "# --- Drawing / debug toggles ---\n",
    "DRAW_GT_BOX = True\n",
    "DRAW_CONTINUITY_DEBUG = True\n",
    "DRAW_GUIDED_ROI_BOX = True\n",
    "DRAW_MOTION_ROI_BOXES = True\n",
    "\n",
    "EXPORT_MOTION_DEBUG_IMAGES = True\n",
    "EXPORT_TILED_DEBUG_IMAGES = True\n",
    "\n",
    "# --- Motion buffering & filtering ---\n",
    "MOTION_BUFFER_INFER_FRAMES = 5      # buffered inference frames used for motion detection\n",
    "NO_DRONE_STREAK_FOR_MOTION = 5      # trigger motion ROIs after this many inference misses\n",
    "MIN_MOVER_W = 25\n",
    "MIN_MOVER_H = 25\n",
    "MAX_MOVER_ROIS = 3\n",
    "\n",
    "# Suppress leaves/trees: require motion to persist across multiple diffs\n",
    "MOTION_MIN_HITS = 3   # minimum number of diff-frames a pixel must be active in\n",
    "MOTION_GLOBAL_COVERAGE_THRESH = 0.35  # if motion mask covers >35% of frame, treat as global motion and discard motion ROIs\n",
    "\n",
    "# --- Confidence thresholds ---\n",
    "BASE_CONF_FULL     = 0.25\n",
    "GUIDED_ROI_CONF    = 0.40\n",
    "LOW_CONF_DETECTION = 0.40\n",
    "\n",
    "# Verification thresholds / sizes\n",
    "VERIFY_HIGH_CONF = 0.60\n",
    "VERIFY_HIGHRES_IMGSZ = 1280\n",
    "\n",
    "# Tiny-object rescue (tiled inference)\n",
    "ENABLE_TINY_RESCUE = True\n",
    "TINY_RESCUE_MIN_WH = 25               # if detected box smaller than this, rescue can trigger\n",
    "TINY_RESCUE_TILED_TILE = 640\n",
    "TINY_RESCUE_TILED_OVERLAP = 0.20\n",
    "\n",
    "# --- Confirmation windows ---\n",
    "CONFIRM_WINDOW = 10\n",
    "CONFIRM_REQUIRED_HITS = 9\n",
    "\n",
    "# Big-confirm window: only detections >=25x25 on ORIGINAL FRAME, unless verified by confirmation guided ROI\n",
    "CONFIRM_MIN_WH_BIG = 25\n",
    "\n",
    "# Warning window: tracks detections >=15x15 (mix of small+big) for later filtering\n",
    "WARNING_MIN_WH = 15\n",
    "\n",
    "# Confirmation guided ROI (secondary inference on same step)\n",
    "ENABLE_CONFIRMATION_GUIDED_ROI_ALL = False  # option 8 (default off)\n",
    "FORCE_VERIFY_SMALL_OR_LOWCONF = True        # option 7 (forced unless baseline)\n",
    "\n",
    "# --- Continuity parameters ---\n",
    "# ExpandedIoU\n",
    "EXPAND_IOU_K = 3.0          # expand previous box by this factor before IoU test\n",
    "EXPAND_IOU_THRESH = 0.10    # IoU threshold after expansion\n",
    "\n",
    "# CenterDistance\n",
    "CENTERDIST_ALPHA = 2.5      # scales with sqrt(area_prev)\n",
    "CENTERDIST_BETA = 30.0      # pixels slack\n",
    "\n",
    "# Kalman\n",
    "KALMAN_Q = 10.0             # process noise\n",
    "KALMAN_R = 50.0             # measurement noise\n",
    "KALMAN_GATE_MAHALANOBIS = 6.0\n",
    "\n",
    "# --- Speed / IO ---\n",
    "MAX_EXPORT_DEBUG_IMAGES = 500   # safety cap\n"
   ],
   "id": "2294f7942797b423"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion ROI method options (choose ONE)\n",
    "\n",
    "Static-camera-friendly:\n",
    "- `MOG2` (adaptive background model)\n",
    "- `KNN`\n",
    "- `RunningAverage`\n",
    "- `TemporalMedian`\n",
    "- `FrameDifference`\n",
    "\n",
    "More tolerant of camera shake / mild motion:\n",
    "- `ORB_Affine_StabilizedDiff` (align frames using ORB features → warp → absdiff)\n",
    "- `ECC_Affine_StabilizedDiff` (align using ECC optimization → warp → absdiff; slower)\n",
    "- `OpticalFlowMagnitude`\n",
    "- `KLT_PointCluster` (tracks points, clusters residual motion)\n",
    "\n",
    "Notes:\n",
    "- If your camera moves a lot, prefer stabilized diff / flow methods.\n",
    "- If motion mask indicates the whole scene moved, the notebook discards motion ROIs and runs full-frame YOLO.\n"
   ],
   "id": "a7b9dfe471c941ff"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cell 5 — Helpers (annotation parsing, geometry, NMS)\n",
    "\n",
    "def read_annotation_file(path: str) -> Dict[int, Optional[Tuple[int,int,int,int]]]:\n",
    "    '''\n",
    "    Annotation format (from your example GOPR5844_002.txt):\n",
    "      - \"frame_id 0\"                          => no drone\n",
    "      - \"frame_id 1 x y w h drone\"            => drone box\n",
    "    Returns dict: frame_id -> (x,y,w,h) or None\n",
    "    '''\n",
    "    gt = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts=line.split()\n",
    "            if len(parts)==2:\n",
    "                fid=int(parts[0]); has=int(parts[1])\n",
    "                gt[fid]=None\n",
    "            elif len(parts)>=7:\n",
    "                fid=int(parts[0]); has=int(parts[1])\n",
    "                if has==0:\n",
    "                    gt[fid]=None\n",
    "                else:\n",
    "                    x=int(float(parts[2])); y=int(float(parts[3])); w=int(float(parts[4])); h=int(float(parts[5]))\n",
    "                    # label is always exactly 'drone' by your rule\n",
    "                    gt[fid]=(x,y,w,h)\n",
    "    return gt\n",
    "\n",
    "def xywh_to_xyxy(box):\n",
    "    x,y,w,h=box\n",
    "    return (x, y, x+w, y+h)\n",
    "\n",
    "def clamp(v, lo, hi):\n",
    "    return max(lo, min(hi, v))\n",
    "\n",
    "def expand_box_xyxy(b, k, W, H):\n",
    "    x1,y1,x2,y2=b\n",
    "    cx=(x1+x2)/2; cy=(y1+y2)/2\n",
    "    w=(x2-x1)*k; h=(y2-y1)*k\n",
    "    nx1=int(clamp(cx-w/2, 0, W-1))\n",
    "    ny1=int(clamp(cy-h/2, 0, H-1))\n",
    "    nx2=int(clamp(cx+w/2, 0, W-1))\n",
    "    ny2=int(clamp(cy+h/2, 0, H-1))\n",
    "    return (nx1,ny1,nx2,ny2)\n",
    "\n",
    "def box_wh_xyxy(b):\n",
    "    x1,y1,x2,y2=b\n",
    "    return (max(0,x2-x1), max(0,y2-y1))\n",
    "\n",
    "def box_area_xyxy(b):\n",
    "    w,h=box_wh_xyxy(b)\n",
    "    return w*h\n",
    "\n",
    "def iou_xyxy(a,b):\n",
    "    ax1,ay1,ax2,ay2=a\n",
    "    bx1,by1,bx2,by2=b\n",
    "    ix1=max(ax1,bx1); iy1=max(ay1,by1)\n",
    "    ix2=min(ax2,bx2); iy2=min(ay2,by2)\n",
    "    iw=max(0,ix2-ix1); ih=max(0,iy2-iy1)\n",
    "    inter=iw*ih\n",
    "    if inter<=0: return 0.0\n",
    "    areaA=(ax2-ax1)*(ay2-ay1); areaB=(bx2-bx1)*(by2-by1)\n",
    "    return inter/(areaA+areaB-inter+1e-9)\n",
    "\n",
    "def nms_xyxy(boxes, scores, iou_thresh=0.5):\n",
    "    if len(boxes)==0:\n",
    "        return []\n",
    "    idx=np.argsort(scores)[::-1]\n",
    "    keep=[]\n",
    "    while idx.size>0:\n",
    "        i=idx[0]\n",
    "        keep.append(i)\n",
    "        if idx.size==1:\n",
    "            break\n",
    "        rest=idx[1:]\n",
    "        ious=np.array([iou_xyxy(boxes[i], boxes[j]) for j in rest])\n",
    "        idx=rest[ious < iou_thresh]\n",
    "    return keep\n"
   ],
   "id": "c289d2af4b034725"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cell 6 — YOLO helpers\n",
    "\n",
    "def load_yolo(weights_path: str) -> YOLO:\n",
    "    model = YOLO(weights_path)\n",
    "    return model\n",
    "\n",
    "def yolo_predict_xyxy(model: YOLO, img_bgr: np.ndarray, conf: float, imgsz: int, max_det: int):\n",
    "    '''\n",
    "    Returns: boxes_xyxy (list of tuples), confs (list of floats)\n",
    "    '''\n",
    "    res = model.predict(\n",
    "        source=img_bgr,\n",
    "        conf=conf,\n",
    "        imgsz=imgsz,\n",
    "        max_det=max_det,\n",
    "        verbose=False\n",
    "    )[0]\n",
    "    boxes=[]\n",
    "    confs=[]\n",
    "    if res.boxes is None or len(res.boxes)==0:\n",
    "        return boxes, confs\n",
    "    xyxy = res.boxes.xyxy.cpu().numpy()\n",
    "    cs   = res.boxes.conf.cpu().numpy()\n",
    "    # class name filtering: label is always drone in GT; but model might have multiple classes.\n",
    "    # We keep ALL predictions and treat \"best\" as the highest conf prediction.\n",
    "    for b,c in zip(xyxy, cs):\n",
    "        x1,y1,x2,y2 = map(float,b)\n",
    "        boxes.append((int(x1),int(y1),int(x2),int(y2)))\n",
    "        confs.append(float(c))\n",
    "    return boxes, confs\n"
   ],
   "id": "554636d98b16b4b9"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cell 7 — Motion ROI extraction (buffered inference frames → 1..3 ROIs)\n",
    "\n",
    "def _to_gray_blur(img):\n",
    "    g=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    g=cv2.GaussianBlur(g,(5,5),0)\n",
    "    return g\n",
    "\n",
    "def _orb_affine(prev_g, curr_g):\n",
    "    orb=cv2.ORB_create(800)\n",
    "    kp1, des1 = orb.detectAndCompute(prev_g, None)\n",
    "    kp2, des2 = orb.detectAndCompute(curr_g, None)\n",
    "    if des1 is None or des2 is None or len(kp1)<8 or len(kp2)<8:\n",
    "        return None\n",
    "    bf=cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches=bf.match(des1, des2)\n",
    "    matches=sorted(matches, key=lambda m: m.distance)[:80]\n",
    "    if len(matches)<8:\n",
    "        return None\n",
    "    pts1=np.float32([kp1[m.queryIdx].pt for m in matches])\n",
    "    pts2=np.float32([kp2[m.trainIdx].pt for m in matches])\n",
    "    M, inl = cv2.estimateAffinePartial2D(pts1, pts2, method=cv2.RANSAC, ransacReprojThreshold=3.0)\n",
    "    return M\n",
    "\n",
    "def _ecc_affine(prev_g, curr_g):\n",
    "    # ECC can be slower but stable when it converges\n",
    "    warp=np.eye(2,3, dtype=np.float32)\n",
    "    criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 1e-4)\n",
    "    try:\n",
    "        cc, warp = cv2.findTransformECC(prev_g, curr_g, warp, cv2.MOTION_AFFINE, criteria, None, 5)\n",
    "        return warp\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _motion_mask_from_pair(prev_bgr, curr_bgr, method):\n",
    "    prev_g=_to_gray_blur(prev_bgr); curr_g=_to_gray_blur(curr_bgr)\n",
    "    H,W=curr_g.shape\n",
    "\n",
    "    if method == \"FrameDifference\":\n",
    "        diff=cv2.absdiff(prev_g, curr_g)\n",
    "        _,mask=cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)\n",
    "        return mask\n",
    "\n",
    "    if method == \"ORB_Affine_StabilizedDiff\":\n",
    "        M=_orb_affine(prev_g, curr_g)\n",
    "        if M is None:\n",
    "            return None\n",
    "        warped=cv2.warpAffine(prev_g, M, (W,H), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)\n",
    "        diff=cv2.absdiff(warped, curr_g)\n",
    "        _,mask=cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)\n",
    "        return mask\n",
    "\n",
    "    if method == \"ECC_Affine_StabilizedDiff\":\n",
    "        M=_ecc_affine(prev_g, curr_g)\n",
    "        if M is None:\n",
    "            return None\n",
    "        warped=cv2.warpAffine(prev_g, M, (W,H), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)\n",
    "        diff=cv2.absdiff(warped, curr_g)\n",
    "        _,mask=cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)\n",
    "        return mask\n",
    "\n",
    "    if method == \"OpticalFlowMagnitude\":\n",
    "        flow=cv2.calcOpticalFlowFarneback(prev_g, curr_g, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "        mag,_=cv2.cartToPolar(flow[...,0], flow[...,1])\n",
    "        mask=(mag>1.5).astype(np.uint8)*255\n",
    "        return mask\n",
    "\n",
    "    # Methods that need a background model will be handled elsewhere\n",
    "    return None\n",
    "\n",
    "def extract_motion_rois(buffer_frames: List[np.ndarray], method: str,\n",
    "                        min_w: int, min_h: int, max_rois: int,\n",
    "                        min_hits: int, global_coverage_thresh: float):\n",
    "    '''\n",
    "    buffer_frames: list of BGR frames sampled at inference times, newest last\n",
    "    Returns: rois_xyxy (list), debug_mask (uint8), global_motion_flag (bool)\n",
    "    '''\n",
    "    if len(buffer_frames) < 2:\n",
    "        return [], None, False\n",
    "\n",
    "    H,W=buffer_frames[-1].shape[:2]\n",
    "\n",
    "    # Background-model methods (static camera) — build a model over buffer frames and produce mask for the last frame.\n",
    "    if method in (\"MOG2\",\"KNN\",\"RunningAverage\",\"TemporalMedian\"):\n",
    "        frames=buffer_frames\n",
    "        if method==\"TemporalMedian\":\n",
    "            stack=np.stack([f.astype(np.uint8) for f in frames], axis=0)\n",
    "            med=np.median(stack, axis=0).astype(np.uint8)\n",
    "            diff=cv2.absdiff(med, frames[-1])\n",
    "            g=_to_gray_blur(diff)\n",
    "            _,mask=cv2.threshold(g, 25, 255, cv2.THRESH_BINARY)\n",
    "        elif method==\"RunningAverage\":\n",
    "            avg=frames[0].astype(np.float32)\n",
    "            alpha=0.1\n",
    "            for fr in frames[1:]:\n",
    "                cv2.accumulateWeighted(fr.astype(np.float32), avg, alpha)\n",
    "            bg=cv2.convertScaleAbs(avg)\n",
    "            diff=cv2.absdiff(bg, frames[-1])\n",
    "            g=_to_gray_blur(diff)\n",
    "            _,mask=cv2.threshold(g, 25, 255, cv2.THRESH_BINARY)\n",
    "        elif method==\"MOG2\":\n",
    "            fgbg=cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=16, detectShadows=True)\n",
    "            mask=None\n",
    "            for fr in frames:\n",
    "                mask=fgbg.apply(fr)\n",
    "        elif method==\"KNN\":\n",
    "            fgbg=cv2.createBackgroundSubtractorKNN(history=500, dist2Threshold=400.0, detectShadows=True)\n",
    "            mask=None\n",
    "            for fr in frames:\n",
    "                mask=fgbg.apply(fr)\n",
    "        else:\n",
    "            mask=None\n",
    "        if mask is None:\n",
    "            return [], None, False\n",
    "        debug_mask=mask.copy()\n",
    "    else:\n",
    "        # Pairwise mask accumulation (robust to leaves by requiring persistence)\n",
    "        masks=[]\n",
    "        for i in range(len(buffer_frames)-1):\n",
    "            m=_motion_mask_from_pair(buffer_frames[i], buffer_frames[i+1], method)\n",
    "            if m is None:\n",
    "                continue\n",
    "            masks.append(m)\n",
    "        if len(masks)==0:\n",
    "            return [], None, False\n",
    "\n",
    "        acc=np.zeros_like(masks[0], dtype=np.uint16)\n",
    "        for m in masks:\n",
    "            acc += (m>0).astype(np.uint16)\n",
    "        # keep pixels that appear in at least min_hits diffs\n",
    "        keep=(acc >= min_hits).astype(np.uint8)*255\n",
    "        debug_mask=keep.copy()\n",
    "\n",
    "    # Morphology to reduce speckle (leaves) and merge regions\n",
    "    k=cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))\n",
    "    m=cv2.morphologyEx(debug_mask, cv2.MORPH_OPEN, k, iterations=1)\n",
    "    m=cv2.morphologyEx(m, cv2.MORPH_CLOSE, k, iterations=2)\n",
    "\n",
    "    coverage = float(np.count_nonzero(m)) / float(H*W + 1e-9)\n",
    "    if coverage > global_coverage_thresh:\n",
    "        return [], m, True\n",
    "\n",
    "    # Contours → ROIs\n",
    "    cnts,_=cv2.findContours(m, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    rois=[]\n",
    "    scored=[]\n",
    "    for c in cnts:\n",
    "        x,y,w,h=cv2.boundingRect(c)\n",
    "        if w < min_w or h < min_h:\n",
    "            continue\n",
    "        area=cv2.contourArea(c)\n",
    "        extent=area / float(w*h + 1e-9)   # leaves often have low extent (wispy)\n",
    "        if extent < 0.15:\n",
    "            continue\n",
    "        rois.append((x,y,x+w,y+h))\n",
    "        scored.append(area)\n",
    "\n",
    "    if len(rois)==0:\n",
    "        return [], m, False\n",
    "\n",
    "    # pick up to max_rois by motion area\n",
    "    order=np.argsort(scored)[::-1]\n",
    "    rois=[rois[i] for i in order[:max_rois]]\n",
    "    return rois, m, False\n"
   ],
   "id": "e8939599400234d2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cell 8 — Tiled inference (slicing) for tiny-object rescue / verification\n",
    "\n",
    "def tiled_inference(model: YOLO, frame_bgr: np.ndarray, conf: float, imgsz: int,\n",
    "                    tile: int, overlap: float, max_det_per_tile: int,\n",
    "                    debug_draw: bool=False):\n",
    "    '''\n",
    "    Runs YOLO on overlapping tiles and merges boxes with NMS.\n",
    "    Returns: merged_boxes_xyxy, merged_confs, debug_image (optional), tiles_xyxy\n",
    "    '''\n",
    "    H,W=frame_bgr.shape[:2]\n",
    "    step=int(tile*(1.0-overlap))\n",
    "    step=max(1, step)\n",
    "\n",
    "    tiles=[]\n",
    "    for y in range(0, H, step):\n",
    "        for x in range(0, W, step):\n",
    "            x2=min(W, x+tile)\n",
    "            y2=min(H, y+tile)\n",
    "            x1=max(0, x2-tile)\n",
    "            y1=max(0, y2-tile)\n",
    "            tiles.append((x1,y1,x2,y2))\n",
    "        if y+tile>=H:\n",
    "            break\n",
    "\n",
    "    all_boxes=[]\n",
    "    all_scores=[]\n",
    "    for (x1,y1,x2,y2) in tiles:\n",
    "        crop=frame_bgr[y1:y2, x1:x2]\n",
    "        b, s = yolo_predict_xyxy(model, crop, conf=conf, imgsz=imgsz, max_det=max_det_per_tile)\n",
    "        for bb,sc in zip(b,s):\n",
    "            bx1,by1,bx2,by2=bb\n",
    "            all_boxes.append((bx1+x1, by1+y1, bx2+x1, by2+y1))\n",
    "            all_scores.append(sc)\n",
    "\n",
    "    if len(all_boxes)==0:\n",
    "        dbg=None\n",
    "        if debug_draw:\n",
    "            dbg=frame_bgr.copy()\n",
    "            for (x1,y1,x2,y2) in tiles:\n",
    "                cv2.rectangle(dbg, (x1,y1),(x2,y2),(255,255,0),1)\n",
    "        return [], [], dbg, tiles\n",
    "\n",
    "    keep=nms_xyxy(all_boxes, all_scores, iou_thresh=0.5)\n",
    "    boxes=[all_boxes[i] for i in keep]\n",
    "    scores=[all_scores[i] for i in keep]\n",
    "\n",
    "    dbg=None\n",
    "    if debug_draw:\n",
    "        dbg=frame_bgr.copy()\n",
    "        for (x1,y1,x2,y2) in tiles:\n",
    "            cv2.rectangle(dbg, (x1,y1),(x2,y2),(255,255,0),1)\n",
    "        for b,s in zip(boxes,scores):\n",
    "            cv2.rectangle(dbg, (b[0],b[1]),(b[2],b[3]),(0,255,255),2)\n",
    "            cv2.putText(dbg, f\"{s:.2f}\", (b[0], max(0,b[1]-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255), 1, cv2.LINE_AA)\n",
    "\n",
    "    return boxes, scores, dbg, tiles\n"
   ],
   "id": "b35930cab6bdd59b"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cell 9 — Continuity gates (selectable)\n",
    "\n",
    "@dataclass\n",
    "class Kalman2D:\n",
    "    # state: [x, y, vx, vy]\n",
    "    x: np.ndarray\n",
    "    P: np.ndarray\n",
    "\n",
    "def kalman_init(cx, cy, q, r):\n",
    "    x=np.array([[cx],[cy],[0.0],[0.0]], dtype=np.float32)\n",
    "    P=np.eye(4, dtype=np.float32)*1000.0\n",
    "    return Kalman2D(x=x, P=P)\n",
    "\n",
    "def kalman_predict(k: Kalman2D, dt: float, q: float):\n",
    "    F=np.array([[1,0,dt,0],\n",
    "                [0,1,0,dt],\n",
    "                [0,0,1,0],\n",
    "                [0,0,0,1]], dtype=np.float32)\n",
    "    Q=np.eye(4, dtype=np.float32)*q\n",
    "    k.x = F @ k.x\n",
    "    k.P = F @ k.P @ F.T + Q\n",
    "    return k\n",
    "\n",
    "def kalman_update(k: Kalman2D, z: np.ndarray, r: float):\n",
    "    H=np.array([[1,0,0,0],\n",
    "                [0,1,0,0]], dtype=np.float32)\n",
    "    R=np.eye(2, dtype=np.float32)*r\n",
    "    y = z - (H @ k.x)\n",
    "    S = H @ k.P @ H.T + R\n",
    "    K = k.P @ H.T @ np.linalg.inv(S)\n",
    "    k.x = k.x + K @ y\n",
    "    I=np.eye(4, dtype=np.float32)\n",
    "    k.P = (I - K @ H) @ k.P\n",
    "    return k, y, S\n",
    "\n",
    "def continuity_accept(prev_box_xyxy, curr_box_xyxy, mode: str, W: int, H: int,\n",
    "                      kalman_state: Optional[Kalman2D], dt: float):\n",
    "    '''\n",
    "    Returns: (accept_bool, debug_shapes_dict, updated_kalman_state)\n",
    "    debug_shapes_dict may contain boxes/circles to draw.\n",
    "    '''\n",
    "    debug={}\n",
    "    if prev_box_xyxy is None or curr_box_xyxy is None:\n",
    "        return True, debug, kalman_state\n",
    "\n",
    "    if mode == \"ExpandedIoU\":\n",
    "        exp=expand_box_xyxy(prev_box_xyxy, EXPAND_IOU_K, W, H)\n",
    "        debug[\"expanded_prev\"]=exp\n",
    "        i=iou_xyxy(exp, curr_box_xyxy)\n",
    "        debug[\"expanded_iou\"]=i\n",
    "        return (i >= EXPAND_IOU_THRESH), debug, kalman_state\n",
    "\n",
    "    if mode == \"CenterDistance\":\n",
    "        px1,py1,px2,py2=prev_box_xyxy\n",
    "        cxp=(px1+px2)/2; cyp=(py1+py2)/2\n",
    "        cx,cy=((curr_box_xyxy[0]+curr_box_xyxy[2])/2, (curr_box_xyxy[1]+curr_box_xyxy[3])/2)\n",
    "        area=max(1.0, box_area_xyxy(prev_box_xyxy))\n",
    "        thr = CENTERDIST_ALPHA*math.sqrt(area) + CENTERDIST_BETA\n",
    "        d=math.hypot(cx-cxp, cy-cyp)\n",
    "        debug[\"center_prev\"]=(int(cxp),int(cyp))\n",
    "        debug[\"center_thr\"]=thr\n",
    "        debug[\"center_curr\"]=(int(cx),int(cy))\n",
    "        return (d <= thr), debug, kalman_state\n",
    "\n",
    "    if mode == \"Kalman\":\n",
    "        # Use curr center as measurement, gate by Mahalanobis distance\n",
    "        cx,cy=((curr_box_xyxy[0]+curr_box_xyxy[2])/2, (curr_box_xyxy[1]+curr_box_xyxy[3])/2)\n",
    "        if kalman_state is None:\n",
    "            kalman_state = kalman_init(cx, cy, KALMAN_Q, KALMAN_R)\n",
    "            debug[\"kalman_pred_center\"]=(int(cx),int(cy))\n",
    "            return True, debug, kalman_state\n",
    "\n",
    "        kalman_state = kalman_predict(kalman_state, dt, KALMAN_Q)\n",
    "        z=np.array([[cx],[cy]], dtype=np.float32)\n",
    "        # Gate before update: compute innovation y and S without applying update\n",
    "        Hm=np.array([[1,0,0,0],[0,1,0,0]], dtype=np.float32)\n",
    "        R=np.eye(2, dtype=np.float32)*KALMAN_R\n",
    "        y = z - (Hm @ kalman_state.x)\n",
    "        S = Hm @ kalman_state.P @ Hm.T + R\n",
    "        # Mahalanobis distance\n",
    "        Sinv=np.linalg.inv(S)\n",
    "        md = float((y.T @ Sinv @ y).squeeze())\n",
    "        pred=(float(kalman_state.x[0]), float(kalman_state.x[1]))\n",
    "        debug[\"kalman_pred_center\"]=(int(pred[0]), int(pred[1]))\n",
    "        debug[\"kalman_md\"]=md\n",
    "\n",
    "        accept = (md <= KALMAN_GATE_MAHALANOBIS)\n",
    "        if accept:\n",
    "            kalman_state,_,_ = kalman_update(kalman_state, z, KALMAN_R)\n",
    "        return accept, debug, kalman_state\n",
    "\n",
    "    return True, debug, kalman_state\n"
   ],
   "id": "887ee4b61d248f99"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cell 10 — Main runner (writes annotated video + logs + summary CSV)\n",
    "\n",
    "def crop_roi_center(frame_bgr, cx, cy, size):\n",
    "    H,W=frame_bgr.shape[:2]\n",
    "    half=size//2\n",
    "    x1=int(clamp(cx-half, 0, W-size))\n",
    "    y1=int(clamp(cy-half, 0, H-size))\n",
    "    x2=x1+size; y2=y1+size\n",
    "    crop=frame_bgr[y1:y2, x1:x2].copy()\n",
    "    return crop, (x1,y1,x2,y2)\n",
    "\n",
    "def best_detection(boxes, confs):\n",
    "    if len(boxes)==0:\n",
    "        return None, None\n",
    "    i=int(np.argmax(confs))\n",
    "    return boxes[i], confs[i]\n",
    "\n",
    "def write_overlay(img, lines):\n",
    "    y=20\n",
    "    for line in lines:\n",
    "        cv2.putText(img, line, (10,y), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (255,255,255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(img, line, (10,y), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0,0,0), 1, cv2.LINE_AA)\n",
    "        y += 18\n",
    "\n",
    "def compute_map50_inference_frames(pred_records, gt_dict, iou_thr=0.5):\n",
    "    '''\n",
    "    pred_records: list of dicts for inference frames only:\n",
    "      {frame_id, conf, box_xyxy or None}\n",
    "    Single-class AP50 across inference frames.\n",
    "    '''\n",
    "    # Build list of predictions with frame association\n",
    "    preds=[]\n",
    "    gts_by_frame={}\n",
    "    for fid,gtxywh in gt_dict.items():\n",
    "        if gtxywh is None:\n",
    "            continue\n",
    "        gts_by_frame[fid]=xywh_to_xyxy(gtxywh)\n",
    "\n",
    "    for r in pred_records:\n",
    "        fid=r[\"frame_id\"]\n",
    "        b=r.get(\"pred_box_xyxy\")\n",
    "        c=r.get(\"pred_conf\")\n",
    "        if b is None:\n",
    "            continue\n",
    "        preds.append((c,fid,b))\n",
    "\n",
    "    if len(preds)==0:\n",
    "        return 0.0\n",
    "\n",
    "    preds.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    matched=set()\n",
    "    tp=[]\n",
    "    fp=[]\n",
    "    for conf,fid,box in preds:\n",
    "        gt=gts_by_frame.get(fid)\n",
    "        if gt is None:\n",
    "            fp.append(1); tp.append(0); continue\n",
    "        i=iou_xyxy(box, gt)\n",
    "        if i>=iou_thr and fid not in matched:\n",
    "            matched.add(fid)\n",
    "            tp.append(1); fp.append(0)\n",
    "        else:\n",
    "            fp.append(1); tp.append(0)\n",
    "\n",
    "    tp=np.cumsum(tp); fp=np.cumsum(fp)\n",
    "    rec = tp / max(1, len(gts_by_frame))\n",
    "    prec = tp / np.maximum(1, (tp+fp))\n",
    "    # 11-point interpolated AP (simple, stable)\n",
    "    ap=0.0\n",
    "    for t in np.linspace(0,1,11):\n",
    "        p = prec[rec>=t].max() if np.any(rec>=t) else 0.0\n",
    "        ap += p/11.0\n",
    "    return float(ap)\n",
    "\n",
    "def run_benchmark():\n",
    "    # Load model + GT\n",
    "    model = load_yolo(MODEL_WEIGHTS)\n",
    "    gt = read_annotation_file(ANNOTATION_PATH) if os.path.exists(ANNOTATION_PATH) else {}\n",
    "\n",
    "    cap=cv2.VideoCapture(VIDEO_PATH)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Cannot open video: {VIDEO_PATH}\")\n",
    "\n",
    "    fps=cap.get(cv2.CAP_PROP_FPS)\n",
    "    if fps <= 0:\n",
    "        fps = 30.0\n",
    "    infer_stride=max(1, int(round(fps / float(INFER_FPS))))\n",
    "\n",
    "    vid_base=os.path.splitext(os.path.basename(VIDEO_PATH))[0]\n",
    "    run_name=f\"{vid_base}__infer{INFER_FPS}fps__motion-{MOTION_METHOD}__cont-{CONTINUITY_GATE_MODE}__verify-{VERIFY_MODE}\"\n",
    "    out_video=os.path.join(OUTPUT_ROOT, run_name + \".mp4\")\n",
    "    out_log=os.path.join(OUTPUT_ROOT, run_name + \"__per_frame_log.csv\")\n",
    "\n",
    "    # debug folders\n",
    "    motion_dir=os.path.join(OUTPUT_ROOT, \"motion_debug\")\n",
    "    tiled_dir=os.path.join(OUTPUT_ROOT, \"tiled_debug\")\n",
    "    os.makedirs(motion_dir, exist_ok=True)\n",
    "    os.makedirs(tiled_dir, exist_ok=True)\n",
    "\n",
    "    W=int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    H=int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc=cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    vw=cv2.VideoWriter(out_video, fourcc, fps, (W,H))\n",
    "\n",
    "    # Master summary CSV\n",
    "    master_csv=os.path.join(OUTPUT_ROOT, \"benchmark_runs_summary.csv\")\n",
    "    master_exists=os.path.exists(master_csv)\n",
    "\n",
    "    # Baseline override logic (your rule): when PREPROCESS_MODE=None and VERIFY_MODE=None, we force baseline.\n",
    "    baseline_mode = (PREPROCESS_MODE==\"None\" and VERIFY_MODE==\"None\")\n",
    "    eff_use_guided = False if baseline_mode else USE_GUIDED_ROI\n",
    "    eff_use_motion = False if baseline_mode else USE_MOTION_ROIS\n",
    "    eff_verify_mode = \"None\" if baseline_mode else VERIFY_MODE\n",
    "    eff_force_verify = False if baseline_mode else FORCE_VERIFY_SMALL_OR_LOWCONF\n",
    "    eff_verify_all = False if baseline_mode else ENABLE_CONFIRMATION_GUIDED_ROI_ALL\n",
    "    eff_tiny_rescue = False if baseline_mode else ENABLE_TINY_RESCUE\n",
    "\n",
    "    # State\n",
    "    infer_idx=0\n",
    "    no_drone_streak=0\n",
    "    last_det_box=None\n",
    "    last_det_conf=None\n",
    "    last_det_source=\"None\"\n",
    "\n",
    "    # Confirmation windows\n",
    "    confirm_hist=[]   # list of booleans (hits) length <=5\n",
    "    warn_hist=[]\n",
    "    confirm_prev_box=None\n",
    "    warn_prev_box=None\n",
    "    kalman_confirm=None\n",
    "    kalman_warn=None\n",
    "\n",
    "    confirmed_events=0\n",
    "    warning_events=0\n",
    "\n",
    "    # Metrics accumulators\n",
    "    per_frame_rows=[]\n",
    "    infer_rows=[]\n",
    "    t_pre=[]; t_inf=[]; t_post=[]\n",
    "\n",
    "    # For stable overlay on every frame\n",
    "    last_overlay = {\n",
    "        \"last_infer_decision\":\"NO DETECTION\",\n",
    "        \"last_infer_source\":\"None\",\n",
    "        \"last_infer_conf\":0.0,\n",
    "        \"last_infer_box\":None,\n",
    "        \"last_infer_rois\":[],\n",
    "        \"last_infer_motion_rois\":[],\n",
    "        \"last_infer_motion_candidates\":[],\n",
    "        \"last_infer_guided_roi\":None,\n",
    "        \"last_infer_continuity_dbg\":{},\n",
    "        \"last_infer_verified\":False,\n",
    "        \"last_infer_verify_mode\":\"None\",\n",
    "        \"last_infer_motion_global\":False,\n",
    "        \"last_infer_yolo_calls\":0,\n",
    "        \"confirm_hits\":0,\n",
    "        \"warn_hits\":0\n",
    "    }\n",
    "\n",
    "    debug_export_count=0\n",
    "\n",
    "    frame_id=0\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "\n",
    "        is_infer = (frame_id % infer_stride == 0)\n",
    "\n",
    "        # Draw GT (optional)\n",
    "        gt_box_xywh = gt.get(frame_id) if gt else None\n",
    "        gt_box_xyxy = xywh_to_xyxy(gt_box_xywh) if (gt_box_xywh is not None) else None\n",
    "        if DRAW_GT_BOX and gt_box_xyxy is not None:\n",
    "            x1,y1,x2,y2=gt_box_xyxy\n",
    "            cv2.rectangle(frame, (x1,y1),(x2,y2),(0,255,0),2)\n",
    "            cv2.putText(frame, \"GT drone\", (x1, max(0,y1-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Defaults for this frame's logs\n",
    "        pred_box=None\n",
    "        pred_conf=0.0\n",
    "        pred_source=\"None\"\n",
    "        verified=False\n",
    "        verify_mode_used=\"None\"\n",
    "        yolo_calls=0\n",
    "        motion_rois=[]\n",
    "        motion_infer_rois=[]\n",
    "        guided_roi_box=None\n",
    "        continuity_debug_shapes={}\n",
    "        motion_global=False\n",
    "\n",
    "        # Motion buffer stores inference frames only\n",
    "        # We'll keep a list in closure-like variable\n",
    "        if frame_id==0:\n",
    "            infer_buffer=[]\n",
    "\n",
    "        if is_infer:\n",
    "            t0=time.perf_counter()\n",
    "\n",
    "            # Update inference buffer\n",
    "            infer_buffer.append(frame.copy())\n",
    "            if len(infer_buffer) > MOTION_BUFFER_INFER_FRAMES:\n",
    "                infer_buffer = infer_buffer[-MOTION_BUFFER_INFER_FRAMES:]\n",
    "\n",
    "            # Decide main input strategy\n",
    "            use_motion_now = eff_use_motion and (no_drone_streak >= NO_DRONE_STREAK_FOR_MOTION) and (len(infer_buffer) >= MOTION_BUFFER_INFER_FRAMES)\n",
    "            use_guided_now = eff_use_guided and (last_det_box is not None) and (not use_motion_now)\n",
    "\n",
    "            inputs=[]   # list of (tag, img, offset_xy) where offset is (ox,oy) to map back to full frame\n",
    "            rois_for_draw=[]\n",
    "\n",
    "            if use_motion_now:\n",
    "                motion_rois, motion_mask, motion_global = extract_motion_rois(\n",
    "                    infer_buffer,\n",
    "                    MOTION_METHOD,\n",
    "                    MIN_MOVER_W, MIN_MOVER_H, MAX_MOVER_ROIS,\n",
    "                    MOTION_MIN_HITS,\n",
    "                    MOTION_GLOBAL_COVERAGE_THRESH\n",
    "                )\n",
    "                if motion_global or len(motion_rois)==0:\n",
    "                    # fallback full frame\n",
    "                    inputs=[(\"FullFrame\", frame, (0,0))]\n",
    "                else:\n",
    "                    # 1..3 ROI crops, centered on ROI center\n",
    "                    for r in motion_rois:\n",
    "                        x1,y1,x2,y2=r\n",
    "                        cx=int((x1+x2)/2); cy=int((y1+y2)/2)\n",
    "                        crop, roi_xyxy = crop_roi_center(frame, cx, cy, ROI_SIZE)\n",
    "                        rois_for_draw.append((\"MotionROI\", roi_xyxy))\n",
    "                        motion_infer_rois.append(roi_xyxy)\n",
    "                        inputs.append((\"MotionROI\", crop, (roi_xyxy[0], roi_xyxy[1])))\n",
    "\n",
    "                    # Export motion debug image\n",
    "                    if EXPORT_MOTION_DEBUG_IMAGES and debug_export_count < MAX_EXPORT_DEBUG_IMAGES:\n",
    "                        dbg=frame.copy()\n",
    "                        if motion_mask is not None:\n",
    "                            small=cv2.resize(motion_mask, (int(W*0.25), int(H*0.25)))\n",
    "                            small=cv2.cvtColor(small, cv2.COLOR_GRAY2BGR)\n",
    "                            dbg[0:small.shape[0], W-small.shape[1]:W] = small\n",
    "                        for _,roi in rois_for_draw:\n",
    "                            cv2.rectangle(dbg, (roi[0],roi[1]),(roi[2],roi[3]),(0,255,255),2)\n",
    "                        cv2.imwrite(os.path.join(motion_dir, f\"{run_name}__infer{infer_idx:06d}.jpg\"), dbg)\n",
    "                        debug_export_count += 1\n",
    "\n",
    "            elif use_guided_now:\n",
    "                # guided ROI around last detection center\n",
    "                x1,y1,x2,y2=last_det_box\n",
    "                cx=int((x1+x2)/2); cy=int((y1+y2)/2)\n",
    "                crop, roi_xyxy = crop_roi_center(frame, cx, cy, ROI_SIZE)\n",
    "                guided_roi_box=roi_xyxy\n",
    "                inputs=[(\"GuidedROI\", crop, (roi_xyxy[0], roi_xyxy[1]))]\n",
    "                rois_for_draw.append((\"GuidedROI\", roi_xyxy))\n",
    "\n",
    "            else:\n",
    "                inputs=[(\"FullFrame\", frame, (0,0))]\n",
    "\n",
    "            t1=time.perf_counter()\n",
    "\n",
    "            # Run YOLO on inputs (full or 1..3 rois). Still counts as ONE inference step.\n",
    "            all_candidates=[]\n",
    "            per_input_detect_counts=[]\n",
    "            for tag, img_in, (ox,oy) in inputs:\n",
    "                conf_th = BASE_CONF_FULL if tag!=\"GuidedROI\" else GUIDED_ROI_CONF\n",
    "                boxes, confs = yolo_predict_xyxy(model, img_in, conf=conf_th, imgsz=ROI_SIZE, max_det=(MAX_ROI_DETECTIONS if tag!=\"FullFrame\" else MAX_FULLFRAME_DETECTIONS))\n",
    "                yolo_calls += 1\n",
    "                per_input_detect_counts.append((tag, len(boxes)))\n",
    "                for b,c in zip(boxes, confs):\n",
    "                    bb=(b[0]+ox, b[1]+oy, b[2]+ox, b[3]+oy)\n",
    "                    all_candidates.append((c, bb, tag))\n",
    "\n",
    "            # Pick best candidate across inputs\n",
    "            if len(all_candidates)>0:\n",
    "                all_candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "                pred_conf, pred_box, pred_source = all_candidates[0]\n",
    "            else:\n",
    "                pred_box=None\n",
    "                pred_conf=0.0\n",
    "                pred_source=\"None\"\n",
    "\n",
    "            # --- Confirmation guided ROI verification on SAME inference step ---\n",
    "            # Required if: low conf OR small box OR verify-all enabled (and we have a detection)\n",
    "            verify_required=False\n",
    "            if pred_box is not None:\n",
    "                pw,ph=box_wh_xyxy(pred_box)\n",
    "                if eff_force_verify and (pred_conf < LOW_CONF_DETECTION or pw < MIN_MOVER_W or ph < MIN_MOVER_H):\n",
    "                    verify_required=True\n",
    "                if eff_verify_all:\n",
    "                    verify_required=True\n",
    "\n",
    "            if pred_box is not None and verify_required:\n",
    "                # Determine mode used (if user chose None but verification is required, we force HighConf)\n",
    "                mode = eff_verify_mode if eff_verify_mode!=\"None\" else \"DoublePassHighConf\"\n",
    "                verify_mode_used=mode\n",
    "\n",
    "                x1,y1,x2,y2=pred_box\n",
    "                cx=int((x1+x2)/2); cy=int((y1+y2)/2)\n",
    "\n",
    "                vt0=time.perf_counter()\n",
    "                if mode==\"DoublePassHighConf\":\n",
    "                    crop, roi_xyxy = crop_roi_center(frame, cx, cy, ROI_SIZE)\n",
    "                    boxes2, confs2 = yolo_predict_xyxy(model, crop, conf=VERIFY_HIGH_CONF, imgsz=ROI_SIZE, max_det=1)\n",
    "                    yolo_calls += 1\n",
    "                    b2,c2=best_detection(boxes2, confs2)\n",
    "                    if b2 is not None:\n",
    "                        b2=(b2[0]+roi_xyxy[0], b2[1]+roi_xyxy[1], b2[2]+roi_xyxy[0], b2[3]+roi_xyxy[1])\n",
    "                        pred_box=b2\n",
    "                        pred_conf=c2\n",
    "                        verified=True\n",
    "                    else:\n",
    "                        # Verification failed → dismiss detection\n",
    "                        pred_box=None\n",
    "                        pred_conf=0.0\n",
    "                        pred_source=\"None\"\n",
    "                        verified=False\n",
    "\n",
    "                elif mode==\"DoublePassHighRes\":\n",
    "                    crop, roi_xyxy = crop_roi_center(frame, cx, cy, ROI_SIZE)\n",
    "                    boxes2, confs2 = yolo_predict_xyxy(model, crop, conf=BASE_CONF_FULL, imgsz=VERIFY_HIGHRES_IMGSZ, max_det=1)\n",
    "                    yolo_calls += 1\n",
    "                    b2,c2=best_detection(boxes2, confs2)\n",
    "                    if b2 is not None:\n",
    "                        b2=(b2[0]+roi_xyxy[0], b2[1]+roi_xyxy[1], b2[2]+roi_xyxy[0], b2[3]+roi_xyxy[1])\n",
    "                        pred_box=b2\n",
    "                        pred_conf=c2\n",
    "                        verified=True\n",
    "                    else:\n",
    "                        pred_box=None\n",
    "                        pred_conf=0.0\n",
    "                        pred_source=\"None\"\n",
    "                        verified=False\n",
    "\n",
    "                elif mode==\"DoublePassTiled\":\n",
    "                    # Tiled verification on full frame (best for tiny objects), save debug image\n",
    "                    boxes2, confs2, dbg, tiles = tiled_inference(model, frame, conf=BASE_CONF_FULL, imgsz=ROI_SIZE,\n",
    "                                                                tile=TINY_RESCUE_TILED_TILE, overlap=TINY_RESCUE_TILED_OVERLAP,\n",
    "                                                                max_det_per_tile=1, debug_draw=EXPORT_TILED_DEBUG_IMAGES)\n",
    "                    yolo_calls += len(tiles)\n",
    "                    b2,c2=best_detection(boxes2, confs2)\n",
    "                    if EXPORT_TILED_DEBUG_IMAGES and dbg is not None and debug_export_count < MAX_EXPORT_DEBUG_IMAGES:\n",
    "                        cv2.imwrite(os.path.join(tiled_dir, f\"{run_name}__infer{infer_idx:06d}.jpg\"), dbg)\n",
    "                        debug_export_count += 1\n",
    "                    if b2 is not None:\n",
    "                        pred_box=b2\n",
    "                        pred_conf=c2\n",
    "                        verified=True\n",
    "                    else:\n",
    "                        pred_box=None\n",
    "                        pred_conf=0.0\n",
    "                        pred_source=\"None\"\n",
    "                        verified=False\n",
    "\n",
    "                vt1=time.perf_counter()\n",
    "                t_post.append((vt1-vt0)*1000.0)\n",
    "            else:\n",
    "                t_post.append(0.0)\n",
    "\n",
    "            # Tiny-object rescue mode (only if enabled and we still have a detection)\n",
    "            if eff_tiny_rescue and pred_box is not None and eff_verify_mode != \"DoublePassTiled\":\n",
    "                pw,ph=box_wh_xyxy(pred_box)\n",
    "                if pw < TINY_RESCUE_MIN_WH or ph < TINY_RESCUE_MIN_WH:\n",
    "                    rt0=time.perf_counter()\n",
    "                    boxes2, confs2, dbg, tiles = tiled_inference(model, frame, conf=BASE_CONF_FULL, imgsz=ROI_SIZE,\n",
    "                                                                tile=TINY_RESCUE_TILED_TILE, overlap=TINY_RESCUE_TILED_OVERLAP,\n",
    "                                                                max_det_per_tile=1, debug_draw=EXPORT_TILED_DEBUG_IMAGES)\n",
    "                    yolo_calls += len(tiles)\n",
    "                    b2,c2=best_detection(boxes2, confs2)\n",
    "                    if EXPORT_TILED_DEBUG_IMAGES and dbg is not None and debug_export_count < MAX_EXPORT_DEBUG_IMAGES:\n",
    "                        cv2.imwrite(os.path.join(tiled_dir, f\"{run_name}__rescue_infer{infer_idx:06d}.jpg\"), dbg)\n",
    "                        debug_export_count += 1\n",
    "                    if b2 is not None and c2 > pred_conf:\n",
    "                        pred_box=b2\n",
    "                        pred_conf=c2\n",
    "                        verified=True\n",
    "                    rt1=time.perf_counter()\n",
    "                    t_post[-1] += (rt1-rt0)*1000.0\n",
    "\n",
    "            t2=time.perf_counter()\n",
    "\n",
    "            # Update streak + last detection (for guided ROI next time)\n",
    "            hit = (pred_box is not None)\n",
    "            if hit:\n",
    "                no_drone_streak=0\n",
    "                last_det_box=pred_box\n",
    "                last_det_conf=pred_conf\n",
    "                last_det_source=pred_source\n",
    "            else:\n",
    "                no_drone_streak += 1\n",
    "\n",
    "            # Confirmation windows logic\n",
    "            # A hit can enter warning if >=15x15. It can enter big-confirm if >=25x25 OR verified True.\n",
    "            big_ok=False\n",
    "            warn_ok=False\n",
    "            if hit:\n",
    "                pw,ph=box_wh_xyxy(pred_box)\n",
    "                warn_ok = (pw>=WARNING_MIN_WH and ph>=WARNING_MIN_WH)\n",
    "                big_ok = (pw>=CONFIRM_MIN_WH_BIG and ph>=CONFIRM_MIN_WH_BIG) or verified\n",
    "\n",
    "            # Continuity gating per window\n",
    "            if warn_ok:\n",
    "                accept, dbg_shapes, kalman_warn = continuity_accept(warn_prev_box, pred_box, CONTINUITY_GATE_MODE, W,H, kalman_warn, dt=1.0/INFER_FPS)\n",
    "                if accept:\n",
    "                    warn_hist.append(True)\n",
    "                    warn_prev_box=pred_box\n",
    "                    continuity_debug_shapes[\"warn\"]=dbg_shapes\n",
    "                else:\n",
    "                    warn_hist.append(False)\n",
    "            else:\n",
    "                warn_hist.append(False)\n",
    "\n",
    "            if big_ok:\n",
    "                accept, dbg_shapes, kalman_confirm = continuity_accept(confirm_prev_box, pred_box, CONTINUITY_GATE_MODE, W,H, kalman_confirm, dt=1.0/INFER_FPS)\n",
    "                if accept:\n",
    "                    confirm_hist.append(True)\n",
    "                    confirm_prev_box=pred_box\n",
    "                    continuity_debug_shapes[\"confirm\"]=dbg_shapes\n",
    "                else:\n",
    "                    confirm_hist.append(False)\n",
    "            else:\n",
    "                confirm_hist.append(False)\n",
    "\n",
    "            confirm_hist=confirm_hist[-CONFIRM_WINDOW:]\n",
    "            warn_hist=warn_hist[-CONFIRM_WINDOW:]\n",
    "\n",
    "            if sum(confirm_hist) >= CONFIRM_REQUIRED_HITS:\n",
    "                confirmed_events += 1\n",
    "                confirm_hist=[]  # reset event window\n",
    "                confirm_prev_box=None\n",
    "                kalman_confirm=None\n",
    "\n",
    "            if sum(warn_hist) >= CONFIRM_REQUIRED_HITS:\n",
    "                warning_events += 1\n",
    "                warn_hist=[]\n",
    "                warn_prev_box=None\n",
    "                kalman_warn=None\n",
    "\n",
    "            # timings\n",
    "            t_pre.append((t1-t0)*1000.0)\n",
    "            t_inf.append((t2-t1)*1000.0)\n",
    "\n",
    "            # record inference frame for AP/mAP\n",
    "            infer_rows.append({\n",
    "                \"frame_id\": frame_id,\n",
    "                \"pred_box_xyxy\": pred_box,\n",
    "                \"pred_conf\": float(pred_conf) if pred_box is not None else None\n",
    "            })\n",
    "\n",
    "            # Store overlay state (used on all subsequent non-infer frames too)\n",
    "            last_overlay.update({\n",
    "                \"last_infer_decision\":\"DETECTION\" if hit else \"NO DETECTION\",\n",
    "                \"last_infer_source\":pred_source,\n",
    "                \"last_infer_conf\":float(pred_conf),\n",
    "                \"last_infer_box\":pred_box,\n",
    "                \"last_infer_motion_rois\": motion_infer_rois,\n",
    "                \"last_infer_motion_candidates\": motion_rois,\n",
    "                \"last_infer_guided_roi\": guided_roi_box,\n",
    "                \"last_infer_continuity_dbg\": continuity_debug_shapes.get(\"confirm\", {}),\n",
    "                \"last_infer_verified\": verified,\n",
    "                \"last_infer_verify_mode\": verify_mode_used,\n",
    "                \"last_infer_motion_global\": motion_global,\n",
    "                \"last_infer_yolo_calls\": yolo_calls,\n",
    "                \"confirm_hits\": sum(confirm_hist),\n",
    "                \"warn_hits\": sum(warn_hist),\n",
    "            })\n",
    "\n",
    "            infer_idx += 1\n",
    "\n",
    "        # Draw predicted box from LAST inference (stable overlay on all frames)\n",
    "        if last_overlay[\"last_infer_box\"] is not None:\n",
    "            b=last_overlay[\"last_infer_box\"]\n",
    "            cv2.rectangle(frame, (b[0],b[1]),(b[2],b[3]),(0,0,255),2)\n",
    "            cv2.putText(frame, f\"Pred drone {last_overlay['last_infer_conf']:.2f}\",\n",
    "                        (b[0], max(0,b[1]-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Draw motion ROIs (last inference)\n",
    "        if DRAW_MOTION_ROI_BOXES:\n",
    "            for r in last_overlay[\"last_infer_motion_rois\"][:MAX_MOVER_ROIS]:\n",
    "                x1,y1,x2,y2=r\n",
    "                cv2.rectangle(frame, (x1,y1),(x2,y2),(0,255,255),2)\n",
    "                cv2.putText(frame, \"Motion ROI\", (x1, min(H-5,y2+18)), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0,255,255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Draw guided ROI (last inference)\n",
    "        if DRAW_GUIDED_ROI_BOX and last_overlay[\"last_infer_guided_roi\"] is not None:\n",
    "            x1,y1,x2,y2=last_overlay[\"last_infer_guided_roi\"]\n",
    "            cv2.rectangle(frame, (x1,y1),(x2,y2),(255,0,0),2)\n",
    "            cv2.putText(frame, \"Guided ROI\", (x1, min(H-5,y2+18)), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (255,0,0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Draw continuity debug (from last inference only, to avoid flicker)\n",
    "        if DRAW_CONTINUITY_DEBUG:\n",
    "            # confirm window shapes\n",
    "            dbg=last_overlay.get(\"last_infer_continuity_dbg\")\n",
    "            if dbg and \"expanded_prev\" in dbg:\n",
    "                x1,y1,x2,y2=dbg[\"expanded_prev\"]\n",
    "                cv2.rectangle(frame, (x1,y1),(x2,y2),(255,255,0),2)\n",
    "                cv2.putText(frame, \"Continuity gate (expanded)\", (x1, max(0,y1-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (255,255,0), 2, cv2.LINE_AA)\n",
    "            if dbg and \"center_prev\" in dbg:\n",
    "                cx,cy=dbg[\"center_prev\"]; thr=int(dbg[\"center_thr\"])\n",
    "                cv2.circle(frame, (cx,cy), thr, (255,255,0), 2)\n",
    "                cv2.putText(frame, \"Continuity gate (center)\", (cx, max(0,cy-thr-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (255,255,0), 2, cv2.LINE_AA)\n",
    "            if dbg and \"kalman_pred_center\" in dbg:\n",
    "                cx,cy=dbg[\"kalman_pred_center\"]\n",
    "                cv2.circle(frame, (cx,cy), 20, (255,255,0), 2)\n",
    "                cv2.putText(frame, \"Continuity gate (kalman)\", (cx, max(0,cy-26)), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (255,255,0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Overlay text (stable structure, no flicker)\n",
    "        tsec = frame_id / fps\n",
    "        overlay_lines = [\n",
    "            f\"Frame: {frame_id}  Time: {tsec:.2f}s  Inference frame: {'YES' if is_infer else 'NO'}  (stride={infer_stride})\",\n",
    "            f\"Baseline mode: {baseline_mode}   Preprocess: {PREPROCESS_MODE}   Use guided ROI: {eff_use_guided}   Use motion ROIs: {eff_use_motion}\",\n",
    "            f\"Motion method: {MOTION_METHOD}   Verify mode: {eff_verify_mode}   Confirm guided ROI all: {eff_verify_all}   Force verify small/lowconf: {eff_force_verify}\",\n",
    "            f\"Last inference: {last_overlay['last_infer_decision']}  Source: {last_overlay['last_infer_source']}  Conf: {last_overlay['last_infer_conf']:.2f}  Verified: {last_overlay['last_infer_verified']} ({last_overlay['last_infer_verify_mode']})\",\n",
    "            f\"No-drone streak: {no_drone_streak}  Motion triggered: {('YES' if (no_drone_streak>=NO_DRONE_STREAK_FOR_MOTION) else 'NO')}  Motion global discarded: {last_overlay['last_infer_motion_global']}\",\n",
    "            f\"Confirm window hits (big >= {CONFIRM_MIN_WH_BIG}px): {last_overlay['confirm_hits']}/{CONFIRM_WINDOW}   Confirmed events: {confirmed_events}\",\n",
    "            f\"Warning window hits (>= {WARNING_MIN_WH}px): {last_overlay['warn_hits']}/{CONFIRM_WINDOW}   Warning events: {warning_events}\",\n",
    "            f\"YOLO calls in last inference step: {last_overlay['last_infer_yolo_calls']}\"\n",
    "        ]\n",
    "        write_overlay(frame, overlay_lines)\n",
    "\n",
    "        # Per-frame log row\n",
    "        row={\n",
    "            \"frame_id\": frame_id,\n",
    "            \"time_s\": tsec,\n",
    "            \"is_infer_frame\": bool(is_infer),\n",
    "            \"gt_has_drone\": bool(gt_box_xyxy is not None) if gt else None,\n",
    "            \"gt_box_xyxy\": gt_box_xyxy,\n",
    "            \"last_pred_has_drone\": bool(last_overlay[\"last_infer_box\"] is not None),\n",
    "            \"last_pred_conf\": last_overlay[\"last_infer_conf\"],\n",
    "            \"last_pred_source\": last_overlay[\"last_infer_source\"],\n",
    "            \"last_pred_verified\": last_overlay[\"last_infer_verified\"],\n",
    "            \"confirmed_events\": confirmed_events,\n",
    "            \"warning_events\": warning_events\n",
    "        }\n",
    "        per_frame_rows.append(row)\n",
    "\n",
    "        vw.write(frame)\n",
    "        frame_id += 1\n",
    "\n",
    "    cap.release()\n",
    "    vw.release()\n",
    "\n",
    "    # Compute summary metrics\n",
    "    # Inference-frame PR metrics:\n",
    "    # Use last inference decision as prediction for that inference frame (already in infer_rows)\n",
    "    y_true=[]; y_pred=[]\n",
    "    confs=[]\n",
    "    for r in infer_rows:\n",
    "        fid=r[\"frame_id\"]\n",
    "        gtbox=gt.get(fid) if gt else None\n",
    "        y_true.append(1 if gtbox is not None else 0)\n",
    "        y_pred.append(1 if r[\"pred_box_xyxy\"] is not None else 0)\n",
    "        if r[\"pred_conf\"] is not None:\n",
    "            confs.append(float(r[\"pred_conf\"]))\n",
    "\n",
    "    tp=sum(1 for yt,yp in zip(y_true,y_pred) if yt==1 and yp==1)\n",
    "    fp=sum(1 for yt,yp in zip(y_true,y_pred) if yt==0 and yp==1)\n",
    "    fn=sum(1 for yt,yp in zip(y_true,y_pred) if yt==1 and yp==0)\n",
    "    precision = tp / max(1, (tp+fp))\n",
    "    recall    = tp / max(1, (tp+fn))\n",
    "    f1 = (2*precision*recall)/max(1e-9, (precision+recall))\n",
    "    map50 = compute_map50_inference_frames(infer_rows, gt, iou_thr=0.5) if gt else 0.0\n",
    "    avg_conf = float(np.mean(confs)) if len(confs)>0 else 0.0\n",
    "\n",
    "    summary={\n",
    "        \"run_name\": run_name,\n",
    "        \"video\": VIDEO_PATH,\n",
    "        \"weights\": MODEL_WEIGHTS,\n",
    "        \"annotation\": ANNOTATION_PATH if os.path.exists(ANNOTATION_PATH) else None,\n",
    "        \"fps\": fps,\n",
    "        \"infer_fps\": INFER_FPS,\n",
    "        \"infer_stride\": infer_stride,\n",
    "        \"baseline_mode\": baseline_mode,\n",
    "        \"preprocess_mode\": PREPROCESS_MODE,\n",
    "        \"use_guided_roi\": eff_use_guided,\n",
    "        \"use_motion_rois\": eff_use_motion,\n",
    "        \"motion_method\": MOTION_METHOD,\n",
    "        \"continuity_gate\": CONTINUITY_GATE_MODE,\n",
    "        \"verify_mode\": eff_verify_mode,\n",
    "        \"verify_all\": eff_verify_all,\n",
    "        \"force_verify_small_lowconf\": eff_force_verify,\n",
    "        \"tiny_rescue\": eff_tiny_rescue,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"mAP50_infer_frames\": map50,\n",
    "        \"avg_pred_conf\": avg_conf,\n",
    "        \"confirmed_events\": confirmed_events,\n",
    "        \"warning_events\": warning_events,\n",
    "        \"avg_pre_ms\": float(np.mean(t_pre)) if len(t_pre)>0 else 0.0,\n",
    "        \"avg_infer_ms\": float(np.mean(t_inf)) if len(t_inf)>0 else 0.0,\n",
    "        \"avg_post_ms\": float(np.mean(t_post)) if len(t_post)>0 else 0.0,\n",
    "    }\n",
    "\n",
    "    # Save per-frame log\n",
    "    pd.DataFrame(per_frame_rows).to_csv(out_log, index=False)\n",
    "\n",
    "    # Append to master summary CSV\n",
    "    df_row=pd.DataFrame([summary])\n",
    "    if master_exists:\n",
    "        df_row.to_csv(master_csv, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df_row.to_csv(master_csv, mode=\"w\", header=True, index=False)\n",
    "\n",
    "    print(\"Saved video:\", out_video)\n",
    "    print(\"Saved per-frame log:\", out_log)\n",
    "    print(\"Updated master summary:\", master_csv)\n",
    "    print(\"\\nSummary:\")\n",
    "    for k,v in summary.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Run it:\n",
    "summary = run_benchmark()\n"
   ],
   "id": "7d942c31fe8ac073"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for your current problem (far-away drones vs birds)\n",
    "\n",
    "- If birds are confidently detected as drone, repetition confirmation alone won't fix it.\n",
    "  You need either:\n",
    "  - verification passes (same YOLO weights) + stricter continuity gating, or\n",
    "  - better training (bird as explicit class, or a separate verifier).\n",
    "\n",
    "- Tiny-object rescue via tiling is the most honest way to improve far-away objects without hallucinating pixels.\n",
    "  Super-resolution can help sometimes, but it also often creates artifacts that detectors misread.\n"
   ],
   "id": "4ea4a148f1d34348"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
