{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "030f4128",
   "metadata": {},
   "source": [
    "# Drone detection video benchmark (YOLO) — baseline + selectable ROI, motion mining, continuity gates, and double-pass verification\n",
    "This notebook runs a **5 FPS (configurable)** inference policy on a video, logs results, and exports an annotated output video.\n",
    "\n",
    "Key properties:\n",
    "- **Every video frame** gets an overlay (no flicker).\n",
    "- Inference happens only on inference frames; intermediate frames reuse the last inference status.\n",
    "- Motion-based ROI mining runs **only after a configurable NO-DRONE streak**.\n",
    "- Guided ROI for the *next* inference (tracking) is separate from confirmation logic.\n",
    "- Confirmation uses a common **K-out-of-N window** with selectable continuity gating.\n",
    "- Optional verification uses the **same YOLO weights** via `VERIFY_MODE`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2c8d7d",
   "metadata": {},
   "source": [
    "## 1) Imports\n",
    "Loads OpenCV, NumPy, Pandas, and Ultralytics YOLO."
   ]
  },
  {
   "cell_type": "code",
   "id": "2abfc59b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T11:19:15.449599Z",
     "start_time": "2026-01-23T11:19:09.063514Z"
    }
   },
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "from dataclasses import dataclass\n",
    "from collections import deque, defaultdict\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Optional: OpenCV contrib bgsegm (not always installed)\n",
    "HAS_BGSEGM = hasattr(cv2, \"bgsegm\")\n",
    "print(\"OpenCV version:\", cv2.__version__)\n",
    "print(\"cv2.bgsegm available:\", HAS_BGSEGM)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV version: 4.12.0\n",
      "cv2.bgsegm available: False\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "c7bfd4fa",
   "metadata": {},
   "source": [
    "## 2) Configuration (your real paths + knobs)\n",
    "### What the confidence parameters mean\n",
    "- `BASE_CONF_FULL`: confidence threshold used for **full-frame** inference and **motion ROI** inference.\n",
    "- `GUIDED_ROI_CONF`: confidence threshold used for **guided ROI** and **verification passes** (usually higher).\n",
    "- `LOW_CONF_DETECTION`: if a detection is below this, it is treated as **low-trust** and (optionally/mandatorily) verified.\n",
    "\n",
    "### Baseline\n",
    "Setting `PRE_MOTION_METHOD = 'None'` and `VERIFY_MODE = 'None'` gives baseline behavior.\n",
    "**Important:** baseline selection does **not** forcibly disable toggles; it only means the selected method is `None`.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2cd930b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T11:19:22.448765Z",
     "start_time": "2026-01-23T11:19:22.433135Z"
    }
   },
   "source": [
    "# --- Your real paths (kept from your latest uploaded notebook) ---\n",
    "MODEL_WEIGHTS = r\"S:\\IntelliJ\\Projects\\ES_Drone_Detection\\runs\\detect\\yolo26\\drone_finetune_full_mixed\\weights\\best.pt\"\n",
    "VIDEO_PATH    = r\"S:\\IntelliJ\\Projects\\ES_Drone_Detection\\video_test\\gopro_006.mp4\"\n",
    "\n",
    "# Annotation path: prefer your local file; else use the uploaded example in this environment.\n",
    "ANNOTATION_PATH_PRIMARY  = r\"S:\\IntelliJ\\Projects\\ES_Drone_Detection\\video_test\\gopro_006.txt\"\n",
    "ANNOTATION_PATH_FALLBACK = r\"/mnt/data/gopro_006.txt\"  # uploaded example\n",
    "ANNOTATION_PATH = ANNOTATION_PATH_PRIMARY if os.path.exists(ANNOTATION_PATH_PRIMARY) else ANNOTATION_PATH_FALLBACK\n",
    "\n",
    "# --- Output folder must be project-root/video_benchmark_outputs/ ---\n",
    "OUTPUT_ROOT = \"video_benchmark_outputs\"\n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "\n",
    "# --- Inference policy ---\n",
    "INFER_FPS = 5                     # inference steps per second (e.g., 5)\n",
    "ROI_SIZE = 640                    # square crop size fed to YOLO\n",
    "BASE_IMGSZ = 640                  # YOLO imgsz for baseline calls (can be raised, slower)\n",
    "HIGHRES_IMGSZ = 1280              # used by VERIFY_MODE=\"DoublePassHighRes\"\n",
    "TILE_SIZE = 640                   # used by tiled inference modes\n",
    "TILE_OVERLAP = 0.20               # 0.2 => 20% overlap\n",
    "TILE_MAX_TILES = 64               # safety cap\n",
    "\n",
    "MAX_FULLFRAME_DETECTIONS = 3\n",
    "MAX_ROI_DETECTIONS = 1\n",
    "\n",
    "# --- Motion ROI mining constraints ---\n",
    "MIN_MOVER_W = 25\n",
    "MIN_MOVER_H = 25\n",
    "MAX_MOVER_ROIS = 3\n",
    "\n",
    "# --- Confidence thresholds ---\n",
    "BASE_CONF_FULL     = 0.25\n",
    "GUIDED_ROI_CONF    = 0.40\n",
    "LOW_CONF_DETECTION = 0.40\n",
    "\n",
    "# --- Verification mode using SAME YOLO weights ---\n",
    "# \"None\" | \"DoublePassHighConf\" | \"DoublePassHighRes\" | \"DoublePassTiled\"\n",
    "VERIFY_MODE = \"None\"\n",
    "\n",
    "# Verification / confirmation guided ROI\n",
    "ENABLE_CONFIRMATION_GUIDED_ROI = True\n",
    "REQUIRE_CONFIRMATION_GUIDED_ROI_FOR_ALL_DETECTIONS = False   # default OFF\n",
    "FORCE_VERIFY_LOWCONF_OR_SMALL = True                         # rule #7\n",
    "\n",
    "# --- Confirmation windows ---\n",
    "# Separate windows:\n",
    "# 1) CONFIRM (big objects only, unless verified)\n",
    "# 2) WARNING (tracks everything, including small)\n",
    "CONFIRM_WINDOW = 10\n",
    "CONFIRM_REQUIRE = 9\n",
    "\n",
    "WARNING_WINDOW = CONFIRM_WINDOW\n",
    "WARNING_REQUIRE = CONFIRM_REQUIRE\n",
    "\n",
    "CONFIRM_COOLDOWN_SEC = 1.0\n",
    "\n",
    "# Size thresholds on ORIGINAL frame coordinates (not ROI coords)\n",
    "MIN_CONFIRM_SIZE_BASE = 15        # rule #12\n",
    "MIN_CONFIRM_SIZE_UNVERIFIED = 25  # rule #9\n",
    "MIN_VERIFY_SIZE = 25              # rule #7\n",
    "MIN_WARNING_SIZE = 1              # warning tracks anything detected\n",
    "\n",
    "# --- Continuity gate selection ---\n",
    "# \"ExpandedIoU\" | \"CenterDistance\" | \"Kalman\"\n",
    "CONTINUITY_GATE = \"ExpandedIoU\"\n",
    "\n",
    "# Expanded IoU gate parameters\n",
    "EXPAND_FACTOR = 3.0               # expand previous box by this factor before IoU\n",
    "\n",
    "# Center-distance gate parameters\n",
    "CENTER_ALPHA = 2.5                # scales with sqrt(area)\n",
    "CENTER_BETA  = 20.0               # extra pixels slack\n",
    "\n",
    "# Kalman gate parameters (pixels)\n",
    "KALMAN_GATE_PX = 120.0\n",
    "\n",
    "SHOW_CONTINUITY_DEBUG = True\n",
    "SHOW_GT_RECT = True\n",
    "\n",
    "# --- Motion mining trigger ---\n",
    "NO_DRONE_STREAK_FOR_MOTION = 5\n",
    "\n",
    "# Motion buffering: number of inference frames stored for motion extraction\n",
    "MOTION_BUFFER_STEPS = 5\n",
    "MOTION_PERSIST_MIN_HITS = 3       # helps suppress leaves (must appear across frames)\n",
    "MOTION_GLOBAL_RATIO_DISCARD = 0.40  # if motion mask covers >40% pixels, treat as global movement\n",
    "\n",
    "# Motion method selection (object motion detector; not stackable with other motion methods)\n",
    "# \"None\" | \"MOG2\" | \"KNN\" | \"RunningAverage\" | \"TemporalMedian\" | \"FrameDifference\"\n",
    "# \"ORB_Affine_StabilizedDiff\" | \"ECC_Affine_StabilizedDiff\" | \"OpticalFlowMagnitude\" | \"KLT_PointCluster\"\n",
    "PRE_MOTION_METHOD = \"None\"\n",
    "\n",
    "# Guided ROI for next inference (tracking)\n",
    "USE_GUIDED_ROI_NEXT_INFER = True\n",
    "\n",
    "# Motion ROI mining toggle (will only run if PRE_MOTION_METHOD != \"None\")\n",
    "USE_MOTION_MINING = True\n",
    "\n",
    "# Tiny-object rescue mode (tiled inference) toggle\n",
    "ENABLE_TINY_RESCUE_TILED = True\n",
    "TINY_RESCUE_MAX_BOX = 25          # if best detected box <= 40x40, optionally try tiled verification\n",
    "\n",
    "# --- Optional exports ---\n",
    "SAVE_VIDEO = True\n",
    "SHOW_PREVIEW = False\n",
    "EXPORT_MOTION_DEBUG_IMAGES = True\n",
    "EXPORT_TILED_DEBUG_IMAGES  = True\n",
    "MAX_EXPORT_DEBUG_IMAGES = 300      # safety cap for each debug type\n",
    "\n",
    "# Annotation label is exactly \"drone\"\n",
    "DRONE_LABEL = \"drone\"\n",
    "\n",
    "print(\"MODEL_WEIGHTS:\", MODEL_WEIGHTS)\n",
    "print(\"VIDEO_PATH:\", VIDEO_PATH)\n",
    "print(\"ANNOTATION_PATH:\", ANNOTATION_PATH)\n",
    "print(\"OUTPUT_ROOT:\", os.path.abspath(OUTPUT_ROOT))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_WEIGHTS: S:\\IntelliJ\\Projects\\ES_Drone_Detection\\runs\\detect\\yolo26\\drone_finetune_full_mixed\\weights\\best.pt\n",
      "VIDEO_PATH: S:\\IntelliJ\\Projects\\ES_Drone_Detection\\video_test\\gopro_006.mp4\n",
      "ANNOTATION_PATH: S:\\IntelliJ\\Projects\\ES_Drone_Detection\\video_test\\gopro_006.txt\n",
      "OUTPUT_ROOT: S:\\IntelliJ\\Projects\\ES_Drone_Detection\\video_benchmark_outputs\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "1fdb1c52",
   "metadata": {},
   "source": [
    "## 3) Utilities\n",
    "Cropping, IoU, drawing helpers, safe averaging, and simple NMS for merging tiled detections."
   ]
  },
  {
   "cell_type": "code",
   "id": "a77981c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T11:19:38.418625Z",
     "start_time": "2026-01-23T11:19:38.402695Z"
    }
   },
   "source": [
    "def safe_mean(xs):\n",
    "    xs = [x for x in xs if x is not None]\n",
    "    return float(sum(xs)/len(xs)) if xs else 0.0\n",
    "\n",
    "def clamp(v, lo, hi):\n",
    "    return max(lo, min(hi, v))\n",
    "\n",
    "def xywh_to_xyxy(x, y, w, h):\n",
    "    return (x, y, x + w, y + h)\n",
    "\n",
    "def xyxy_to_xywh(x1, y1, x2, y2):\n",
    "    return (x1, y1, x2 - x1, y2 - y1)\n",
    "\n",
    "def box_area_xyxy(b):\n",
    "    x1,y1,x2,y2 = b\n",
    "    return max(0, x2-x1)*max(0, y2-y1)\n",
    "\n",
    "def iou_xyxy(a, b):\n",
    "    ax1,ay1,ax2,ay2 = a\n",
    "    bx1,by1,bx2,by2 = b\n",
    "    ix1,iy1 = max(ax1,bx1), max(ay1,by1)\n",
    "    ix2,iy2 = min(ax2,bx2), min(ay2,by2)\n",
    "    iw,ih = max(0, ix2-ix1), max(0, iy2-iy1)\n",
    "    inter = iw*ih\n",
    "    ua = box_area_xyxy(a) + box_area_xyxy(b) - inter\n",
    "    return inter/ua if ua > 0 else 0.0\n",
    "\n",
    "def center_of(b):\n",
    "    x1,y1,x2,y2=b\n",
    "    return ((x1+x2)/2.0, (y1+y2)/2.0)\n",
    "\n",
    "def expand_box(b, factor, W, H):\n",
    "    x1,y1,x2,y2=b\n",
    "    cx,cy = center_of(b)\n",
    "    w = (x2-x1)*factor\n",
    "    h = (y2-y1)*factor\n",
    "    nx1 = clamp(int(cx - w/2), 0, W-1)\n",
    "    ny1 = clamp(int(cy - h/2), 0, H-1)\n",
    "    nx2 = clamp(int(cx + w/2), 0, W-1)\n",
    "    ny2 = clamp(int(cy + h/2), 0, H-1)\n",
    "    if nx2 <= nx1: nx2 = min(W-1, nx1+1)\n",
    "    if ny2 <= ny1: ny2 = min(H-1, ny1+1)\n",
    "    return (nx1,ny1,nx2,ny2)\n",
    "\n",
    "def crop_square_center(frame, cx, cy, size):\n",
    "    H,W = frame.shape[:2]\n",
    "    half = size // 2\n",
    "    x1 = clamp(int(cx - half), 0, W - 1)\n",
    "    y1 = clamp(int(cy - half), 0, H - 1)\n",
    "    x2 = clamp(int(x1 + size), 1, W)\n",
    "    y2 = clamp(int(y1 + size), 1, H)\n",
    "    # if near border, shift back to keep square\n",
    "    if x2 - x1 < size:\n",
    "        x1 = clamp(x2 - size, 0, W - size)\n",
    "        x2 = x1 + size\n",
    "    if y2 - y1 < size:\n",
    "        y1 = clamp(y2 - size, 0, H - size)\n",
    "        y2 = y1 + size\n",
    "    x1,y1,x2,y2 = int(x1),int(y1),int(x2),int(y2)\n",
    "    crop = frame[y1:y2, x1:x2].copy()\n",
    "    return crop, (x1,y1,x2,y2)\n",
    "\n",
    "def draw_box(img, b, color, thickness=2, label=None):\n",
    "    x1,y1,x2,y2 = map(int,b)\n",
    "    cv2.rectangle(img, (x1,y1), (x2,y2), color, thickness)\n",
    "    if label:\n",
    "        cv2.putText(img, label, (x1, max(0,y1-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.55, color, 2, cv2.LINE_AA)\n",
    "\n",
    "def nms_xyxy(boxes, scores, iou_thr=0.6):\n",
    "    if not boxes:\n",
    "        return []\n",
    "    idxs = np.argsort(scores)[::-1]\n",
    "    keep=[]\n",
    "    while len(idxs)>0:\n",
    "        i = idxs[0]\n",
    "        keep.append(i)\n",
    "        rest = []\n",
    "        for j in idxs[1:]:\n",
    "            if iou_xyxy(boxes[i], boxes[j]) < iou_thr:\n",
    "                rest.append(j)\n",
    "        idxs = np.array(rest, dtype=int)\n",
    "    return keep"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "cd7acb9e",
   "metadata": {},
   "source": [
    "## 4) Annotation parser (your format)\n",
    "Your `.txt` file format is **one line per frame**:\n",
    "- `frame_id 0` → no drone\n",
    "- `frame_id 1 x y w h drone` → one drone box\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "73bb321f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T11:19:42.042354Z",
     "start_time": "2026-01-23T11:19:42.024356Z"
    }
   },
   "source": [
    "def load_annotations(path: str):\n",
    "    gt = {}\n",
    "    if not path or not os.path.exists(path):\n",
    "        print(\"Annotation file not found:\", path)\n",
    "        return gt\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts=line.split()\n",
    "            # expected: frame_id 0\n",
    "            # or: frame_id 1 x y w h drone\n",
    "            try:\n",
    "                frame_id = int(parts[0])\n",
    "            except:\n",
    "                continue\n",
    "            if len(parts) >= 2 and parts[1] == \"0\":\n",
    "                gt[frame_id] = None\n",
    "            elif len(parts) >= 7:\n",
    "                # frame_id 1 x y w h label\n",
    "                x = float(parts[2]); y=float(parts[3]); w=float(parts[4]); h=float(parts[5])\n",
    "                label = parts[6].strip()\n",
    "                if label == DRONE_LABEL:\n",
    "                    gt[frame_id] = xywh_to_xyxy(x,y,w,h)\n",
    "                else:\n",
    "                    gt[frame_id] = None\n",
    "            else:\n",
    "                gt[frame_id] = None\n",
    "    return gt\n",
    "\n",
    "GT = load_annotations(ANNOTATION_PATH)\n",
    "print(\"GT frames loaded:\", len(GT))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT frames loaded: 1351\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "01481ad3",
   "metadata": {},
   "source": [
    "## 5) Continuity gates (choose in config)\n",
    "These gates decide whether a detection belongs to the **same target** across inference frames.\n",
    "- **ExpandedIoU**: expands previous bbox (2–3×) before IoU. Good for low FPS.\n",
    "- **CenterDistance**: compares centers with a size-scaled radius.\n",
    "- **Kalman**: predicts next center using a constant-velocity Kalman filter, then gates by distance.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "78417f36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T11:19:53.209414Z",
     "start_time": "2026-01-23T11:19:53.178125Z"
    }
   },
   "source": [
    "class ExpandedIoUGate:\n",
    "    def __init__(self, expand_factor=3.0, iou_thr=0.05):\n",
    "        self.expand_factor = expand_factor\n",
    "        self.iou_thr = iou_thr\n",
    "    def pass_gate(self, prev_box, curr_box, W, H):\n",
    "        prev_exp = expand_box(prev_box, self.expand_factor, W, H)\n",
    "        return iou_xyxy(prev_exp, curr_box) >= self.iou_thr, {\"prev_exp\": prev_exp}\n",
    "\n",
    "class CenterDistanceGate:\n",
    "    def __init__(self, alpha=2.5, beta=20.0):\n",
    "        self.alpha=alpha\n",
    "        self.beta=beta\n",
    "    def pass_gate(self, prev_box, curr_box, W, H):\n",
    "        pc = center_of(prev_box)\n",
    "        cc = center_of(curr_box)\n",
    "        dist = math.hypot(pc[0]-cc[0], pc[1]-cc[1])\n",
    "        area = max(1.0, box_area_xyxy(prev_box))\n",
    "        thr = self.alpha*math.sqrt(area) + self.beta\n",
    "        return dist <= thr, {\"pc\": pc, \"thr\": thr}\n",
    "\n",
    "class KalmanGate:\n",
    "    # 2D constant velocity: state [x,y,vx,vy]\n",
    "    def __init__(self, gate_px=120.0):\n",
    "        self.gate_px = gate_px\n",
    "        self.x = None\n",
    "        self.P = None\n",
    "        self.last_t = None\n",
    "        # fixed noise\n",
    "        self.Q_base = 5.0\n",
    "        self.R = np.diag([25.0, 25.0]).astype(np.float32)\n",
    "    def reset(self):\n",
    "        self.x=None; self.P=None; self.last_t=None\n",
    "    def predict(self, dt):\n",
    "        F = np.array([[1,0,dt,0],\n",
    "                      [0,1,0,dt],\n",
    "                      [0,0,1,0],\n",
    "                      [0,0,0,1]], dtype=np.float32)\n",
    "        q = self.Q_base\n",
    "        Q = np.diag([q,q,q,q]).astype(np.float32)\n",
    "        self.x = F @ self.x\n",
    "        self.P = F @ self.P @ F.T + Q\n",
    "        return self.x.copy()\n",
    "    def update(self, z):\n",
    "        # z: [x,y]\n",
    "        H = np.array([[1,0,0,0],\n",
    "                      [0,1,0,0]], dtype=np.float32)\n",
    "        y = z - (H @ self.x)\n",
    "        S = H @ self.P @ H.T + self.R\n",
    "        K = self.P @ H.T @ np.linalg.inv(S)\n",
    "        self.x = self.x + K @ y\n",
    "        I = np.eye(4, dtype=np.float32)\n",
    "        self.P = (I - K @ H) @ self.P\n",
    "    def pass_gate(self, prev_box, curr_box, W, H, t=None):\n",
    "        # Initialize on first call\n",
    "        cc = np.array(center_of(curr_box), dtype=np.float32)\n",
    "        if self.x is None:\n",
    "            self.x = np.array([cc[0], cc[1], 0.0, 0.0], dtype=np.float32)\n",
    "            self.P = np.eye(4, dtype=np.float32)*100.0\n",
    "            self.last_t = t\n",
    "            return True, {\"pred\": (cc[0], cc[1]), \"gate\": self.gate_px}\n",
    "        dt = 1.0 if self.last_t is None or t is None else max(1e-3, float(t - self.last_t))\n",
    "        pred = self.predict(dt)\n",
    "        pred_xy = (float(pred[0]), float(pred[1]))\n",
    "        dist = math.hypot(pred_xy[0]-cc[0], pred_xy[1]-cc[1])\n",
    "        ok = dist <= self.gate_px\n",
    "        # update if accepted, else keep prediction (do not latch onto noise)\n",
    "        if ok:\n",
    "            self.update(cc)\n",
    "            self.last_t = t\n",
    "        return ok, {\"pred\": pred_xy, \"gate\": self.gate_px}\n",
    "\n",
    "def make_gate():\n",
    "    if CONTINUITY_GATE == \"ExpandedIoU\":\n",
    "        return ExpandedIoUGate(expand_factor=EXPAND_FACTOR, iou_thr=CONFIRM_IOU_CONTINUITY if 'CONFIRM_IOU_CONTINUITY' in globals() else 0.05)\n",
    "    if CONTINUITY_GATE == \"CenterDistance\":\n",
    "        return CenterDistanceGate(alpha=CENTER_ALPHA, beta=CENTER_BETA)\n",
    "    if CONTINUITY_GATE == \"Kalman\":\n",
    "        return KalmanGate(gate_px=KALMAN_GATE_PX)\n",
    "    return ExpandedIoUGate(expand_factor=EXPAND_FACTOR, iou_thr=0.05)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "f85e1653",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T11:20:07.623175Z",
     "start_time": "2026-01-23T11:20:07.606117Z"
    }
   },
   "source": [
    "# IoU threshold used by ExpandedIoU gate after expansion\n",
    "CONFIRM_IOU_CONTINUITY = 0.05"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "d6204d57",
   "metadata": {},
   "source": [
    "## 6) Motion detectors (select one)\n",
    "Motion ROI mining runs only after `NO_DRONE_STREAK_FOR_MOTION` consecutive inference steps without an accepted drone.\n",
    "It outputs **1 to 3** ROIs and runs YOLO on each ROI (max 3 YOLO calls per inference step).\n",
    "\n",
    "Leaf/trees suppression used here:\n",
    "- temporal persistence across buffered inference frames (`MOTION_PERSIST_MIN_HITS`)\n",
    "- discard if the motion mask covers too much of the frame (`MOTION_GLOBAL_RATIO_DISCARD`) → treat as global movement\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2b8991d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T11:20:14.272183Z",
     "start_time": "2026-01-23T11:20:14.232793Z"
    }
   },
   "source": [
    "def to_gray_blur(frame):\n",
    "    g = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    g = cv2.GaussianBlur(g, (5,5), 0)\n",
    "    return g\n",
    "\n",
    "def motion_mask_framediff(prev_g, curr_g, thr=20):\n",
    "    d = cv2.absdiff(prev_g, curr_g)\n",
    "    _, m = cv2.threshold(d, thr, 255, cv2.THRESH_BINARY)\n",
    "    return m\n",
    "\n",
    "def motion_mask_optflow(prev_g, curr_g, mag_thr=1.5):\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_g, curr_g, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    mag, _ = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
    "    m = (mag > mag_thr).astype(np.uint8)*255\n",
    "    return m\n",
    "\n",
    "def estimate_affine_orb(prev_g, curr_g):\n",
    "    orb = cv2.ORB_create(500)\n",
    "    kp1, des1 = orb.detectAndCompute(prev_g, None)\n",
    "    kp2, des2 = orb.detectAndCompute(curr_g, None)\n",
    "    if des1 is None or des2 is None or len(kp1) < 10 or len(kp2) < 10:\n",
    "        return None\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(des1, des2)\n",
    "    if len(matches) < 10:\n",
    "        return None\n",
    "    matches = sorted(matches, key=lambda x: x.distance)[:80]\n",
    "    pts1 = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1,1,2)\n",
    "    pts2 = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1,1,2)\n",
    "    M, _ = cv2.estimateAffinePartial2D(pts1, pts2, method=cv2.RANSAC, ransacReprojThreshold=3.0)\n",
    "    return M\n",
    "\n",
    "def motion_mask_orb_stabilized(prev_g, curr_g, thr=20):\n",
    "    H,W = curr_g.shape[:2]\n",
    "    M = estimate_affine_orb(prev_g, curr_g)\n",
    "    if M is None:\n",
    "        return motion_mask_framediff(prev_g, curr_g, thr=thr)\n",
    "    warped = cv2.warpAffine(prev_g, M, (W,H), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)\n",
    "    return motion_mask_framediff(warped, curr_g, thr=thr)\n",
    "\n",
    "def motion_mask_ecc_stabilized(prev_g, curr_g, thr=20):\n",
    "    H,W = curr_g.shape[:2]\n",
    "    warp = np.eye(2,3, dtype=np.float32)\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 50, 1e-4)\n",
    "    try:\n",
    "        cc, warp = cv2.findTransformECC(prev_g, curr_g, warp, cv2.MOTION_AFFINE, criteria)\n",
    "        warped = cv2.warpAffine(prev_g, warp, (W,H), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)\n",
    "        return motion_mask_framediff(warped, curr_g, thr=thr)\n",
    "    except:\n",
    "        return motion_mask_framediff(prev_g, curr_g, thr=thr)\n",
    "\n",
    "def klt_clusters(prev_g, curr_g, max_pts=200):\n",
    "    p0 = cv2.goodFeaturesToTrack(prev_g, maxCorners=max_pts, qualityLevel=0.01, minDistance=7, blockSize=7)\n",
    "    if p0 is None:\n",
    "        return np.zeros_like(prev_g, dtype=np.uint8)\n",
    "    p1, st, err = cv2.calcOpticalFlowPyrLK(prev_g, curr_g, p0, None, winSize=(21,21), maxLevel=3,\n",
    "                                           criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.01))\n",
    "    if p1 is None:\n",
    "        return np.zeros_like(prev_g, dtype=np.uint8)\n",
    "    good0 = p0[st==1]\n",
    "    good1 = p1[st==1]\n",
    "    if len(good0) < 10:\n",
    "        return np.zeros_like(prev_g, dtype=np.uint8)\n",
    "    # global motion\n",
    "    flow = good1 - good0\n",
    "    med = np.median(flow, axis=0)\n",
    "    res = flow - med\n",
    "    mag = np.linalg.norm(res, axis=1)\n",
    "    # keep points with residual motion\n",
    "    keep = mag > 2.0\n",
    "    mask = np.zeros_like(prev_g, dtype=np.uint8)\n",
    "    for (x,y) in good1[keep]:\n",
    "        cv2.circle(mask, (int(x),int(y)), 5, 255, -1)\n",
    "    return mask\n",
    "\n",
    "class MotionMiner:\n",
    "    def __init__(self, method: str):\n",
    "        self.method = method\n",
    "        self.mog2 = None\n",
    "        self.knn = None\n",
    "        self.runavg = None\n",
    "        self.runavg_alpha = 0.02\n",
    "        self.temporal = deque(maxlen=max(5, MOTION_BUFFER_STEPS))\n",
    "    def _init_bg(self):\n",
    "        if self.method == \"MOG2\" and self.mog2 is None:\n",
    "            self.mog2 = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=16, detectShadows=True)\n",
    "        if self.method == \"KNN\" and self.knn is None:\n",
    "            self.knn = cv2.createBackgroundSubtractorKNN(history=500, dist2Threshold=400.0, detectShadows=True)\n",
    "    def mask_from_pair(self, prev_g, curr_g):\n",
    "        if self.method == \"FrameDifference\":\n",
    "            return motion_mask_framediff(prev_g, curr_g, thr=20)\n",
    "        if self.method == \"ORB_Affine_StabilizedDiff\":\n",
    "            return motion_mask_orb_stabilized(prev_g, curr_g, thr=20)\n",
    "        if self.method == \"ECC_Affine_StabilizedDiff\":\n",
    "            return motion_mask_ecc_stabilized(prev_g, curr_g, thr=20)\n",
    "        if self.method == \"OpticalFlowMagnitude\":\n",
    "            return motion_mask_optflow(prev_g, curr_g, mag_thr=1.5)\n",
    "        if self.method == \"KLT_PointCluster\":\n",
    "            return klt_clusters(prev_g, curr_g)\n",
    "        return motion_mask_framediff(prev_g, curr_g, thr=20)\n",
    "    def update_and_get_mask(self, frame_bgr):\n",
    "        g = to_gray_blur(frame_bgr)\n",
    "        self.temporal.append(g)\n",
    "        self._init_bg()\n",
    "        if self.method in (\"MOG2\",\"KNN\"):\n",
    "            if self.method == \"MOG2\":\n",
    "                fg = self.mog2.apply(frame_bgr)\n",
    "            else:\n",
    "                fg = self.knn.apply(frame_bgr)\n",
    "            # clean noise\n",
    "            k = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n",
    "            fg = cv2.morphologyEx(fg, cv2.MORPH_OPEN, k)\n",
    "            return fg\n",
    "        if self.method == \"RunningAverage\":\n",
    "            if self.runavg is None:\n",
    "                self.runavg = g.astype(np.float32)\n",
    "                return np.zeros_like(g, dtype=np.uint8)\n",
    "            cv2.accumulateWeighted(g.astype(np.float32), self.runavg, self.runavg_alpha)\n",
    "            bg = cv2.convertScaleAbs(self.runavg)\n",
    "            return motion_mask_framediff(bg, g, thr=20)\n",
    "        if self.method == \"TemporalMedian\":\n",
    "            if len(self.temporal) < self.temporal.maxlen:\n",
    "                return np.zeros_like(g, dtype=np.uint8)\n",
    "            stack = np.stack(list(self.temporal), axis=0)\n",
    "            med = np.median(stack, axis=0).astype(np.uint8)\n",
    "            return motion_mask_framediff(med, g, thr=20)\n",
    "        # pair-based methods need prev frame\n",
    "        if len(self.temporal) < 2:\n",
    "            return np.zeros_like(g, dtype=np.uint8)\n",
    "        prev = self.temporal[-2]\n",
    "        curr = self.temporal[-1]\n",
    "        return self.mask_from_pair(prev, curr)\n",
    "\n",
    "def rois_from_mask(mask, W, H, min_w, min_h, max_rois):\n",
    "    # morphology to connect\n",
    "    k = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))\n",
    "    m = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, k)\n",
    "    m = cv2.morphologyEx(m, cv2.MORPH_OPEN, k)\n",
    "    contours, _ = cv2.findContours(m, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    rois=[]\n",
    "    for c in contours:\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        if w < min_w or h < min_h:\n",
    "            continue\n",
    "        rois.append((x,y,x+w,y+h, w*h))\n",
    "    rois.sort(key=lambda t: t[4], reverse=True)\n",
    "    return [(x1,y1,x2,y2) for (x1,y1,x2,y2,_) in rois[:max_rois]], m"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "4fac3eca",
   "metadata": {},
   "source": [
    "## 7) YOLO wrappers + tiled inference\n",
    "Tiled inference increases effective resolution for small objects by running YOLO over overlapping tiles and merging detections.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "fb5ce681",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T11:20:17.469518Z",
     "start_time": "2026-01-23T11:20:17.066439Z"
    }
   },
   "source": [
    "def load_model(weights):\n",
    "    model = YOLO(weights)\n",
    "    return model\n",
    "\n",
    "model = load_model(MODEL_WEIGHTS)\n",
    "names = model.model.names if hasattr(model, \"model\") and hasattr(model.model, \"names\") else {}\n",
    "print(\"Model names:\", names)\n",
    "\n",
    "# Find class id for \"drone\" by exact label match preference\n",
    "def find_drone_class_id(names_dict):\n",
    "    if isinstance(names_dict, dict):\n",
    "        for k,v in names_dict.items():\n",
    "            if str(v).strip().lower() == DRONE_LABEL:\n",
    "                return int(k)\n",
    "    # fallback: if only 1 class, assume it is drone\n",
    "    if isinstance(names_dict, dict) and len(names_dict)==1:\n",
    "        return int(list(names_dict.keys())[0])\n",
    "    return None\n",
    "\n",
    "DRONE_CLASS_ID = find_drone_class_id(names)\n",
    "print(\"DRONE_CLASS_ID:\", DRONE_CLASS_ID)\n",
    "\n",
    "def run_yolo_on_image(img_bgr, conf, max_det, imgsz):\n",
    "    t0 = time.perf_counter()\n",
    "    res = model.predict(img_bgr, conf=conf, max_det=max_det, imgsz=imgsz, verbose=False)[0]\n",
    "    infer_ms = (time.perf_counter()-t0)*1000.0\n",
    "    boxes=[]\n",
    "    confs=[]\n",
    "    clss=[]\n",
    "    if res.boxes is not None and len(res.boxes) > 0:\n",
    "        b = res.boxes.xyxy.cpu().numpy()\n",
    "        c = res.boxes.conf.cpu().numpy()\n",
    "        k = res.boxes.cls.cpu().numpy()\n",
    "        for i in range(len(b)):\n",
    "            boxes.append(tuple(map(float, b[i])))\n",
    "            confs.append(float(c[i]))\n",
    "            clss.append(int(k[i]))\n",
    "    return boxes, confs, clss, infer_ms\n",
    "\n",
    "def filter_drone(boxes, confs, clss):\n",
    "    out=[]\n",
    "    for b,cf,cl in zip(boxes, confs, clss):\n",
    "        if DRONE_CLASS_ID is None or cl == DRONE_CLASS_ID:\n",
    "            out.append((b, cf))\n",
    "    out.sort(key=lambda t: t[1], reverse=True)\n",
    "    return out\n",
    "\n",
    "def tiled_inference(frame_bgr, conf, max_det, imgsz, tile_size=640, overlap=0.2, max_tiles=64):\n",
    "    H,W = frame_bgr.shape[:2]\n",
    "    step = int(tile_size * (1.0 - overlap))\n",
    "    step = max(1, step)\n",
    "    boxes_all=[]\n",
    "    confs_all=[]\n",
    "    tiles=[]\n",
    "    for y in range(0, H, step):\n",
    "        for x in range(0, W, step):\n",
    "            x2 = min(W, x+tile_size)\n",
    "            y2 = min(H, y+tile_size)\n",
    "            x1 = max(0, x2 - tile_size)\n",
    "            y1 = max(0, y2 - tile_size)\n",
    "            tiles.append((x1,y1,x2,y2))\n",
    "            if len(tiles) >= max_tiles:\n",
    "                break\n",
    "        if len(tiles) >= max_tiles:\n",
    "            break\n",
    "    infer_ms_total=0.0\n",
    "    for (x1,y1,x2,y2) in tiles:\n",
    "        tile = frame_bgr[y1:y2, x1:x2]\n",
    "        b, c, k, ms = run_yolo_on_image(tile, conf=conf, max_det=max_det, imgsz=imgsz)\n",
    "        infer_ms_total += ms\n",
    "        dets = filter_drone(b,c,k)\n",
    "        for (bb, cf) in dets:\n",
    "            bx1,by1,bx2,by2 = bb\n",
    "            boxes_all.append((bx1+x1, by1+y1, bx2+x1, by2+y1))\n",
    "            confs_all.append(cf)\n",
    "    if not boxes_all:\n",
    "        return [], [], infer_ms_total, tiles\n",
    "    keep = nms_xyxy(boxes_all, confs_all, iou_thr=0.6)\n",
    "    boxes_n = [boxes_all[i] for i in keep]\n",
    "    confs_n = [confs_all[i] for i in keep]\n",
    "    # sort by conf\n",
    "    order = np.argsort(confs_n)[::-1]\n",
    "    boxes_n = [boxes_n[i] for i in order]\n",
    "    confs_n = [confs_n[i] for i in order]\n",
    "    return boxes_n, confs_n, infer_ms_total, tiles"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model names: {0: 'drone'}\n",
      "DRONE_CLASS_ID: 0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "23970b9e",
   "metadata": {},
   "source": [
    "## 8) Main pipeline\n",
    "Rules implemented:\n",
    "- Overlay updates on inference frames, but is drawn on **all frames** (no flicker).\n",
    "- Motion mining: after `NO_DRONE_STREAK_FOR_MOTION`, run 1–3 ROI inferences and draw all yellow ROIs.\n",
    "- Guided ROI (cyan) is used for the **next inference** after an accepted detection.\n",
    "- Confirmation-guided ROI (magenta) is optional post-pass on the **same inference**.\n",
    "- Continuity gate is selectable and can be visualized.\n",
    "- If motion indicates global scene movement, motion ROIs are discarded (full-frame used).\n",
    "- One inference step = one hit max (even if multiple ROI inferences run).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "0d3e3049",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T11:20:24.341216Z",
     "start_time": "2026-01-23T11:20:24.308004Z"
    }
   },
   "source": [
    "@dataclass\n",
    "class StepDecision:\n",
    "    has_drone: bool\n",
    "    primary_box: Optional[Tuple[float,float,float,float]]\n",
    "    primary_conf: float\n",
    "    source: str                      # \"FullFrame\" | \"GuidedROI\" | \"MotionROI\" | \"Tiled\" | \"Verify\"\n",
    "    yolo_calls: int\n",
    "    boxes_all: List[Tuple[float,float,float,float]]  # all drone boxes (orig coords)\n",
    "    confs_all: List[float]\n",
    "    verified: bool\n",
    "    verify_source: str\n",
    "\n",
    "def box_w_h(b):\n",
    "    x1,y1,x2,y2=b\n",
    "    return max(0,x2-x1), max(0,y2-y1)\n",
    "\n",
    "def is_small_box(b, min_side):\n",
    "    w,h = box_w_h(b)\n",
    "    return (w < min_side) or (h < min_side)\n",
    "\n",
    "def size_ok_for_confirm(b, verified):\n",
    "    w,h = box_w_h(b)\n",
    "    if w < MIN_CONFIRM_SIZE_BASE or h < MIN_CONFIRM_SIZE_BASE:\n",
    "        return False\n",
    "    if (w < MIN_CONFIRM_SIZE_UNVERIFIED or h < MIN_CONFIRM_SIZE_UNVERIFIED) and (not verified):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def make_run_id(video_path):\n",
    "    base = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    return f\"{base}__infer{INFER_FPS}fps__motion-{PRE_MOTION_METHOD}__verify-{VERIFY_MODE}\"\n",
    "\n",
    "RUN_ID = make_run_id(VIDEO_PATH)\n",
    "OUT_VIDEO_PATH = os.path.join(OUTPUT_ROOT, f\"{RUN_ID}.mp4\")\n",
    "PER_FRAME_CSV  = os.path.join(OUTPUT_ROOT, f\"{RUN_ID}__per_frame_log.csv\")\n",
    "MASTER_SUMMARY_CSV = os.path.join(OUTPUT_ROOT, \"benchmark_runs_summary.csv\")\n",
    "\n",
    "MOTION_DEBUG_DIR = os.path.join(OUTPUT_ROOT, \"motion_debug\")\n",
    "TILED_DEBUG_DIR  = os.path.join(OUTPUT_ROOT, \"tiled_debug\")\n",
    "os.makedirs(MOTION_DEBUG_DIR, exist_ok=True)\n",
    "os.makedirs(TILED_DEBUG_DIR, exist_ok=True)\n",
    "\n",
    "def decide_inference_inputs(frame_bgr, W, H, forced_guided_rect, do_motion, motion_rois):\n",
    "    inputs=[]\n",
    "    # guided ROI for NEXT inference (cyan), if available\n",
    "    if forced_guided_rect is not None:\n",
    "        x1,y1,x2,y2 = forced_guided_rect\n",
    "        crop = frame_bgr[y1:y2, x1:x2]\n",
    "        inputs.append((\"GuidedROI\", crop, forced_guided_rect))\n",
    "        return inputs  # guided ROI ignores motion/baseline in that inference step\n",
    "\n",
    "    if do_motion and motion_rois:\n",
    "        for r in motion_rois:\n",
    "            x1,y1,x2,y2 = r\n",
    "            cx,cy = (x1+x2)/2.0, (y1+y2)/2.0\n",
    "            crop, rect = crop_square_center(frame_bgr, cx, cy, ROI_SIZE)\n",
    "            inputs.append((\"MotionROI\", crop, rect))\n",
    "        return inputs\n",
    "\n",
    "    # baseline full frame\n",
    "    inputs.append((\"FullFrame\", frame_bgr, (0,0,W,H)))\n",
    "    return inputs\n",
    "\n",
    "def verification_pass(frame_bgr, primary_box, mode):\n",
    "    # returns: (verified_bool, verified_box, verified_conf, verify_calls, verify_ms, debug_tiles)\n",
    "    H,W = frame_bgr.shape[:2]\n",
    "    cx,cy = center_of(primary_box)\n",
    "    debug_tiles=None\n",
    "\n",
    "    if mode == \"DoublePassHighConf\":\n",
    "        crop, rect = crop_square_center(frame_bgr, cx, cy, ROI_SIZE)\n",
    "        b,c,k,ms = run_yolo_on_image(crop, conf=GUIDED_ROI_CONF, max_det=MAX_ROI_DETECTIONS, imgsz=BASE_IMGSZ)\n",
    "        dets = filter_drone(b,c,k)\n",
    "        if dets:\n",
    "            vb, vconf = dets[0]\n",
    "            # map to orig\n",
    "            x1,y1,x2,y2 = rect\n",
    "            vb_o = (vb[0]+x1, vb[1]+y1, vb[2]+x1, vb[3]+y1)\n",
    "            return True, vb_o, vconf, 1, ms, debug_tiles\n",
    "        return False, None, 0.0, 1, ms, debug_tiles\n",
    "\n",
    "    if mode == \"DoublePassHighRes\":\n",
    "        # larger crop + higher imgsz\n",
    "        big = min(max(W,H), ROI_SIZE*2)\n",
    "        crop, rect = crop_square_center(frame_bgr, cx, cy, big)\n",
    "        b,c,k,ms = run_yolo_on_image(crop, conf=GUIDED_ROI_CONF, max_det=MAX_ROI_DETECTIONS, imgsz=HIGHRES_IMGSZ)\n",
    "        dets = filter_drone(b,c,k)\n",
    "        if dets:\n",
    "            vb, vconf = dets[0]\n",
    "            x1,y1,x2,y2 = rect\n",
    "            vb_o = (vb[0]+x1, vb[1]+y1, vb[2]+x1, vb[3]+y1)\n",
    "            return True, vb_o, vconf, 1, ms, debug_tiles\n",
    "        return False, None, 0.0, 1, ms, debug_tiles\n",
    "\n",
    "    if mode == \"DoublePassTiled\":\n",
    "        big = min(max(W,H), ROI_SIZE*2)\n",
    "        crop, rect = crop_square_center(frame_bgr, cx, cy, big)\n",
    "        boxes, confs, ms, tiles = tiled_inference(crop, conf=GUIDED_ROI_CONF, max_det=MAX_ROI_DETECTIONS, imgsz=BASE_IMGSZ,\n",
    "                                                  tile_size=TILE_SIZE, overlap=TILE_OVERLAP, max_tiles=TILE_MAX_TILES)\n",
    "        debug_tiles = [(tx+rect[0], ty+rect[1], tx2+rect[0], ty2+rect[1]) for (tx,ty,tx2,ty2) in tiles]\n",
    "        if boxes:\n",
    "            vb_o = (boxes[0][0]+rect[0], boxes[0][1]+rect[1], boxes[0][2]+rect[0], boxes[0][3]+rect[1])\n",
    "            return True, vb_o, float(confs[0]), len(tiles), ms, debug_tiles\n",
    "        return False, None, 0.0, len(tiles), ms, debug_tiles\n",
    "\n",
    "    return False, None, 0.0, 0, 0.0, debug_tiles\n",
    "\n",
    "def build_motion_rois(motion_masks, W, H):\n",
    "    # temporal persistence to reduce leaves\n",
    "    if not motion_masks:\n",
    "        return [], None, 0.0\n",
    "    acc = np.zeros_like(motion_masks[0], dtype=np.uint16)\n",
    "    for m in motion_masks:\n",
    "        acc += (m > 0).astype(np.uint16)\n",
    "    # keep pixels that moved in at least MOTION_PERSIST_MIN_HITS masks\n",
    "    keep = (acc >= MOTION_PERSIST_MIN_HITS).astype(np.uint8)*255\n",
    "\n",
    "    motion_ratio = float(np.count_nonzero(keep)) / float(W*H)\n",
    "    return keep, acc, motion_ratio\n",
    "\n",
    "def ensure_master_summary_header(path):\n",
    "    if os.path.exists(path):\n",
    "        return\n",
    "    cols = [\n",
    "        \"run_id\",\"video\",\"infer_fps\",\"pre_motion_method\",\"use_motion\",\"use_guided_next\",\"verify_mode\",\n",
    "        \"confirm_window\",\"confirm_require\",\"warning_window\",\"warning_require\",\n",
    "        \"base_conf_full\",\"guided_roi_conf\",\"low_conf_detection\",\n",
    "        \"precision_infer\",\"recall_infer\",\"ap50_infer\",\n",
    "        \"avg_conf_infer\",\n",
    "        \"avg_pre_ms\",\"avg_infer_ms\",\"avg_post_ms\",\n",
    "        \"confirmed_events\",\"warning_events\",\n",
    "        \"notes\"\n",
    "    ]\n",
    "    with open(path,\"w\",newline=\"\",encoding=\"utf-8\") as f:\n",
    "        w=csv.writer(f); w.writerow(cols)\n",
    "\n",
    "ensure_master_summary_header(MASTER_SUMMARY_CSV)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "5cb4d0ee",
   "metadata": {},
   "source": [
    "## 9) Run benchmark\n",
    "This cell:\n",
    "- Reads the video\n",
    "- Runs inference at `INFER_FPS`\n",
    "- Exports output video + per-frame CSV\n",
    "- Appends a row to `video_benchmark_outputs/benchmark_runs_summary.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "fbae05c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T11:21:20.965156Z",
     "start_time": "2026-01-23T11:20:30.305830Z"
    }
   },
   "source": [
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"Cannot open video: {VIDEO_PATH}\")\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "infer_stride = max(1, int(round(fps / float(INFER_FPS))))\n",
    "print(\"Video FPS:\", fps, \"Frames:\", frame_count, \"Infer stride:\", infer_stride)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "writer = None\n",
    "if SAVE_VIDEO:\n",
    "    writer = cv2.VideoWriter(OUT_VIDEO_PATH, fourcc, fps, (W,H))\n",
    "\n",
    "# State\n",
    "motion_miner = MotionMiner(PRE_MOTION_METHOD) if PRE_MOTION_METHOD != \"None\" else None\n",
    "motion_masks_buffer = deque(maxlen=MOTION_BUFFER_STEPS)\n",
    "infer_frames_gray_buffer = deque(maxlen=MOTION_BUFFER_STEPS)\n",
    "\n",
    "forced_guided_rect_next = None   # cyan (next inference)\n",
    "no_drone_streak = 0\n",
    "\n",
    "gate = make_gate()\n",
    "kalman_gate = gate if isinstance(gate, KalmanGate) else None\n",
    "\n",
    "# confirmation windows\n",
    "confirm_hits = deque(maxlen=CONFIRM_WINDOW)    # entries: (hit_bool, box, verified, t)\n",
    "warning_hits = deque(maxlen=WARNING_WINDOW)\n",
    "\n",
    "last_confirm_time = -1e9\n",
    "last_warning_time = -1e9\n",
    "confirmed_events = 0\n",
    "warning_events = 0\n",
    "\n",
    "# Counters\n",
    "counts = defaultdict(int)\n",
    "pre_ms_list=[]\n",
    "infer_ms_list=[]\n",
    "post_ms_list=[]\n",
    "avg_conf_list=[]\n",
    "\n",
    "# For AP50 on inference frames\n",
    "infer_eval = []  # list of dicts: {frame_id, pred_box, pred_conf, gt_box}\n",
    "\n",
    "# overlay state persists across non-infer frames (no flicker)\n",
    "overlay_state = {\n",
    "    \"last_infer_frame\": -1,\n",
    "    \"last_infer_time_s\": 0.0,\n",
    "    \"last_mode\": \"Baseline\",\n",
    "    \"last_source\": \"FullFrame\",\n",
    "    \"last_has_drone\": False,\n",
    "    \"last_conf\": 0.0,\n",
    "    \"last_box\": None,\n",
    "    \"last_verified\": False,\n",
    "    \"last_verify_mode\": \"None\",\n",
    "    \"last_yolo_calls\": 0,\n",
    "    \"motion_rois_used\": 0,\n",
    "    \"motion_rois_rects\": [],\n",
    "    \"guided_rect_used\": None,\n",
    "    \"confirm_guided_rect\": None,\n",
    "    \"continuity_debug\": None,\n",
    "    \"tiles_debug\": None,\n",
    "}\n",
    "\n",
    "# Debug export counters\n",
    "motion_dbg_saved = 0\n",
    "tiled_dbg_saved  = 0\n",
    "\n",
    "# per-frame csv\n",
    "csv_cols = [\n",
    "    \"frame_id\",\"time_s\",\"is_infer_frame\",\n",
    "    \"mode\",\"source\",\"has_drone\",\"primary_conf\",\"primary_box\",\n",
    "    \"verified\",\"verify_mode\",\"yolo_calls\",\n",
    "    \"gt_box\",\n",
    "    \"confirm_state\",\"warning_state\",\n",
    "    \"confirmed_events\",\"warning_events\",\n",
    "    \"no_drone_streak\",\n",
    "    \"count_full_hits\",\"count_guided_hits\",\"count_motion_hits\",\"count_verified_hits\"\n",
    "]\n",
    "csv_rows=[]\n",
    "\n",
    "def overlay_lines(frame_id, time_s):\n",
    "    # Use overlay_state values, updated on inference frames only\n",
    "    lines=[]\n",
    "    lines.append(f\"Frame: {frame_id}  Time: {time_s:.2f}s\")\n",
    "    lines.append(f\"Inference frame: {'YES' if (frame_id % infer_stride == 0) else 'NO'}  (infer_fps={INFER_FPS}, stride={infer_stride})\")\n",
    "    lines.append(f\"Mode: {overlay_state['last_mode']} | Source: {overlay_state['last_source']} | YOLO calls (this inference): {overlay_state['last_yolo_calls']}\")\n",
    "    lines.append(f\"Drone detected: {overlay_state['last_has_drone']} | Conf: {overlay_state['last_conf']:.3f} | Verified: {overlay_state['last_verified']} ({overlay_state['last_verify_mode']})\")\n",
    "    lines.append(f\"Hits so far: FullFrame={counts['full_hit']} GuidedNext={counts['guided_hit']} MotionROI={counts['motion_hit']} Verified={counts['verified_hit']}\")\n",
    "    lines.append(f\"Confirmed events: {confirmed_events} | Warning events: {warning_events} | No-drone streak: {no_drone_streak}\")\n",
    "    return lines\n",
    "\n",
    "def compute_ap50(infer_eval_rows):\n",
    "    # One GT per frame at most. Compute AP50 like detection AP:\n",
    "    # sort predictions by conf, TP if IoU>=0.5 with GT on that frame, else FP.\n",
    "    # frames without GT are ignored for recall denominator but included for precision via FP.\n",
    "    preds=[]\n",
    "    gt_frames=set()\n",
    "    for r in infer_eval_rows:\n",
    "        if r[\"gt_box\"] is not None:\n",
    "            gt_frames.add(r[\"frame_id\"])\n",
    "        if r[\"pred_box\"] is not None:\n",
    "            preds.append((r[\"pred_conf\"], r[\"frame_id\"], r[\"pred_box\"], r[\"gt_box\"]))\n",
    "    preds.sort(key=lambda t: t[0], reverse=True)\n",
    "    matched=set()\n",
    "    tps=[]\n",
    "    fps=[]\n",
    "    tp=0\n",
    "    fp=0\n",
    "    for conf, fid, pb, gb in preds:\n",
    "        if gb is not None and fid not in matched and iou_xyxy(pb, gb) >= 0.5:\n",
    "            tp += 1\n",
    "            matched.add(fid)\n",
    "            tps.append(tp); fps.append(fp)\n",
    "        else:\n",
    "            fp += 1\n",
    "            tps.append(tp); fps.append(fp)\n",
    "    if not gt_frames:\n",
    "        return 0.0\n",
    "    precisions=[]\n",
    "    recalls=[]\n",
    "    for i in range(len(preds)):\n",
    "        p = tps[i] / max(1, (tps[i]+fps[i]))\n",
    "        r = tps[i] / len(gt_frames)\n",
    "        precisions.append(p); recalls.append(r)\n",
    "    # 11-point interp AP\n",
    "    ap=0.0\n",
    "    for t in np.linspace(0,1,11):\n",
    "        pmax = 0.0\n",
    "        for p,r in zip(precisions, recalls):\n",
    "            if r >= t:\n",
    "                pmax = max(pmax, p)\n",
    "        ap += pmax\n",
    "    ap /= 11.0\n",
    "    return float(ap)\n",
    "\n",
    "def update_window(window_deque, hit_ok, box, verified, t, W, H):\n",
    "    # continuity: if previous hit exists, gate must pass; else accept\n",
    "    dbg=None\n",
    "    if hit_ok and box is not None:\n",
    "        prev = None\n",
    "        for e in reversed(window_deque):\n",
    "            if e[0] and e[1] is not None:\n",
    "                prev = e[1]\n",
    "                break\n",
    "        if prev is not None:\n",
    "            if isinstance(gate, KalmanGate):\n",
    "                ok, info = gate.pass_gate(prev, box, W, H, t=t)\n",
    "                dbg=info\n",
    "            else:\n",
    "                ok, info = gate.pass_gate(prev, box, W, H)\n",
    "                dbg=info\n",
    "            if not ok:\n",
    "                hit_ok = False\n",
    "    window_deque.append((hit_ok, box, verified, t, dbg))\n",
    "    return hit_ok, dbg\n",
    "\n",
    "frame_id=0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    t_s = frame_id / fps\n",
    "\n",
    "    is_infer = (frame_id % infer_stride == 0)\n",
    "\n",
    "    # --- draw overlay later; we first maybe update overlay_state if infer ---\n",
    "    if is_infer:\n",
    "        pre_t0 = time.perf_counter()\n",
    "\n",
    "        # Update motion buffer (for motion ROI mining)\n",
    "        g = to_gray_blur(frame)\n",
    "        infer_frames_gray_buffer.append(g)\n",
    "\n",
    "        # Update motion miner mask buffer if enabled and method chosen\n",
    "        current_motion_mask = None\n",
    "        if motion_miner is not None:\n",
    "            current_motion_mask = motion_miner.update_and_get_mask(frame)\n",
    "            motion_masks_buffer.append(current_motion_mask)\n",
    "\n",
    "        # decide whether to motion-mine this inference\n",
    "        do_motion = (USE_MOTION_MINING and motion_miner is not None and no_drone_streak >= NO_DRONE_STREAK_FOR_MOTION)\n",
    "        motion_rois=[]\n",
    "        motion_keep_mask=None\n",
    "        motion_ratio=0.0\n",
    "        motion_acc=None\n",
    "\n",
    "        if do_motion and len(motion_masks_buffer) >= 2:\n",
    "            keep_mask, acc, ratio = build_motion_rois(list(motion_masks_buffer), W, H)\n",
    "            motion_keep_mask = keep_mask\n",
    "            motion_acc = acc\n",
    "            motion_ratio = ratio\n",
    "            if motion_ratio > MOTION_GLOBAL_RATIO_DISCARD:\n",
    "                # global movement -> discard motion ROIs\n",
    "                do_motion = False\n",
    "            else:\n",
    "                rois, cleaned = rois_from_mask(motion_keep_mask, W, H, MIN_MOVER_W, MIN_MOVER_H, MAX_MOVER_ROIS)\n",
    "                motion_rois = rois\n",
    "\n",
    "        # Prepare inputs (baseline/guided/motion)\n",
    "        inputs = decide_inference_inputs(frame, W, H, forced_guided_rect_next if USE_GUIDED_ROI_NEXT_INFER else None, do_motion, motion_rois)\n",
    "\n",
    "        forced_guided_rect_next = None  # consumed\n",
    "\n",
    "        pre_ms = (time.perf_counter()-pre_t0)*1000.0\n",
    "\n",
    "        # Run YOLO on selected inputs\n",
    "        infer_calls = 0\n",
    "        infer_ms = 0.0\n",
    "        drone_boxes=[]\n",
    "        drone_confs=[]\n",
    "        primary_source=\"FullFrame\"\n",
    "\n",
    "        # For visualization on this inference frame\n",
    "        overlay_state[\"motion_rois_rects\"] = []\n",
    "        overlay_state[\"guided_rect_used\"] = None\n",
    "        overlay_state[\"confirm_guided_rect\"] = None\n",
    "        overlay_state[\"continuity_debug\"] = None\n",
    "        overlay_state[\"tiles_debug\"] = None\n",
    "\n",
    "        for idx,(src, img, rect) in enumerate(inputs):\n",
    "            if src == \"FullFrame\":\n",
    "                conf = BASE_CONF_FULL\n",
    "                max_det = MAX_FULLFRAME_DETECTIONS\n",
    "                imgsz = BASE_IMGSZ\n",
    "            elif src == \"GuidedROI\":\n",
    "                conf = BASE_CONF_FULL  # first pass threshold; verification uses GUIDED_ROI_CONF\n",
    "                max_det = MAX_ROI_DETECTIONS\n",
    "                imgsz = BASE_IMGSZ\n",
    "                overlay_state[\"guided_rect_used\"] = rect\n",
    "            elif src == \"MotionROI\":\n",
    "                conf = BASE_CONF_FULL\n",
    "                max_det = MAX_ROI_DETECTIONS\n",
    "                imgsz = BASE_IMGSZ\n",
    "                overlay_state[\"motion_rois_rects\"].append(rect)\n",
    "            else:\n",
    "                conf = BASE_CONF_FULL; max_det = MAX_ROI_DETECTIONS; imgsz = BASE_IMGSZ\n",
    "\n",
    "            b,c,k,ms = run_yolo_on_image(img, conf=conf, max_det=max_det, imgsz=imgsz)\n",
    "            infer_calls += 1\n",
    "            infer_ms += ms\n",
    "\n",
    "            dets = filter_drone(b,c,k)\n",
    "            if dets:\n",
    "                # map to original coords\n",
    "                rx1,ry1,rx2,ry2 = rect\n",
    "                for (bb, cf) in dets:\n",
    "                    bx1,by1,bx2,by2 = bb\n",
    "                    drone_boxes.append((bx1+rx1, by1+ry1, bx2+rx1, by2+ry1))\n",
    "                    drone_confs.append(cf)\n",
    "                if primary_source == \"FullFrame\":\n",
    "                    primary_source = src\n",
    "\n",
    "        # pick primary\n",
    "        has_drone = len(drone_boxes) > 0\n",
    "        primary_box = None\n",
    "        primary_conf = 0.0\n",
    "        if has_drone:\n",
    "            best_i = int(np.argmax(drone_confs))\n",
    "            primary_box = drone_boxes[best_i]\n",
    "            primary_conf = float(drone_confs[best_i])\n",
    "\n",
    "        verified=False\n",
    "        verify_calls=0\n",
    "        verify_ms=0.0\n",
    "        verify_source=\"None\"\n",
    "        verify_tiles=None\n",
    "\n",
    "        # Decide if we must verify on same inference (confirmation guided ROI)\n",
    "        must_verify = False\n",
    "        if has_drone and primary_box is not None:\n",
    "            if (primary_conf < LOW_CONF_DETECTION) or is_small_box(primary_box, MIN_VERIFY_SIZE):\n",
    "                must_verify = True\n",
    "        if REQUIRE_CONFIRMATION_GUIDED_ROI_FOR_ALL_DETECTIONS and has_drone:\n",
    "            must_verify = True\n",
    "\n",
    "        # Do verification if enabled/forced\n",
    "        if has_drone and primary_box is not None and ( (ENABLE_CONFIRMATION_GUIDED_ROI and must_verify) or (FORCE_VERIFY_LOWCONF_OR_SMALL and must_verify) ):\n",
    "            post_t0 = time.perf_counter()\n",
    "            # If VERIFY_MODE==\"None\" but verification is mandatory, default to HighConf\n",
    "            mode = VERIFY_MODE if VERIFY_MODE != \"None\" else \"DoublePassHighConf\"\n",
    "            verified, vbox, vconf, vcalls, vms, vtiles = verification_pass(frame, primary_box, mode)\n",
    "            verify_calls += vcalls\n",
    "            verify_ms += vms\n",
    "            verify_tiles = vtiles\n",
    "            overlay_state[\"confirm_guided_rect\"] = expand_box(primary_box, 2.0, W, H)\n",
    "            overlay_state[\"tiles_debug\"] = verify_tiles\n",
    "            verify_source = mode\n",
    "\n",
    "            if not verified:\n",
    "                # dismiss original detection as false positive\n",
    "                has_drone = False\n",
    "                primary_box = None\n",
    "                primary_conf = 0.0\n",
    "                drone_boxes=[]\n",
    "                drone_confs=[]\n",
    "            else:\n",
    "                # replace with verified box/conf if available\n",
    "                if vbox is not None:\n",
    "                    primary_box = vbox\n",
    "                if vconf is not None:\n",
    "                    primary_conf = float(vconf)\n",
    "                counts[\"verified_hit\"] += 1\n",
    "            post_ms = (time.perf_counter()-post_t0)*1000.0\n",
    "        else:\n",
    "            post_ms = 0.0\n",
    "\n",
    "        # Tiny rescue tiled (only when enabled and still no drone AND motion mining didn't trigger)\n",
    "        if (not has_drone) and ENABLE_TINY_RESCUE_TILED and (no_drone_streak >= NO_DRONE_STREAK_FOR_MOTION) and (PRE_MOTION_METHOD == \"None\"):\n",
    "            post_t0 = time.perf_counter()\n",
    "            boxes_t, confs_t, ms_t, tiles = tiled_inference(frame, conf=BASE_CONF_FULL, max_det=MAX_FULLFRAME_DETECTIONS, imgsz=BASE_IMGSZ,\n",
    "                                                            tile_size=TILE_SIZE, overlap=TILE_OVERLAP, max_tiles=TILE_MAX_TILES)\n",
    "            infer_calls += len(tiles)\n",
    "            infer_ms += ms_t\n",
    "            if boxes_t:\n",
    "                has_drone = True\n",
    "                primary_box = boxes_t[0]\n",
    "                primary_conf = float(confs_t[0])\n",
    "                primary_source = \"Tiled\"\n",
    "                overlay_state[\"tiles_debug\"] = tiles\n",
    "            post_ms += (time.perf_counter()-post_t0)*1000.0\n",
    "\n",
    "        # Update guided ROI for next inference if enabled and detection accepted\n",
    "        if has_drone and primary_box is not None and USE_GUIDED_ROI_NEXT_INFER:\n",
    "            cx,cy = center_of(primary_box)\n",
    "            _, rect = crop_square_center(frame, cx, cy, ROI_SIZE)\n",
    "            forced_guided_rect_next = rect\n",
    "\n",
    "        # Update streak\n",
    "        if has_drone:\n",
    "            no_drone_streak = 0\n",
    "        else:\n",
    "            no_drone_streak += 1\n",
    "\n",
    "        # Update windows (warning + confirm)\n",
    "        hit_t = t_s\n",
    "        warn_hit_ok = has_drone and primary_box is not None\n",
    "        warn_hit_ok, warn_dbg = update_window(warning_hits, warn_hit_ok, primary_box, verified, hit_t, W, H)\n",
    "\n",
    "        conf_hit_ok = False\n",
    "        if has_drone and primary_box is not None:\n",
    "            conf_hit_ok = size_ok_for_confirm(primary_box, verified)\n",
    "        conf_hit_ok, conf_dbg = update_window(confirm_hits, conf_hit_ok, primary_box, verified, hit_t, W, H)\n",
    "\n",
    "        overlay_state[\"continuity_debug\"] = conf_dbg if conf_dbg is not None else warn_dbg\n",
    "\n",
    "        # event triggering\n",
    "        def count_hits(window):\n",
    "            return sum(1 for e in window if e[0])\n",
    "\n",
    "        now = t_s\n",
    "        if (now - last_confirm_time) >= CONFIRM_COOLDOWN_SEC:\n",
    "            if count_hits(confirm_hits) >= CONFIRM_REQUIRE:\n",
    "                confirmed_events += 1\n",
    "                last_confirm_time = now\n",
    "\n",
    "        if (now - last_warning_time) >= CONFIRM_COOLDOWN_SEC:\n",
    "            if count_hits(warning_hits) >= WARNING_REQUIRE:\n",
    "                warning_events += 1\n",
    "                last_warning_time = now\n",
    "\n",
    "        # Update hit counters by source\n",
    "        if has_drone:\n",
    "            if primary_source == \"FullFrame\": counts[\"full_hit\"] += 1\n",
    "            elif primary_source == \"GuidedROI\": counts[\"guided_hit\"] += 1\n",
    "            elif primary_source == \"MotionROI\": counts[\"motion_hit\"] += 1\n",
    "            elif primary_source == \"Tiled\": counts[\"tiled_hit\"] += 1\n",
    "\n",
    "        # Evaluate AP50 inputs (inference frames only)\n",
    "        gt_box = GT.get(frame_id, None)\n",
    "        infer_eval.append({\n",
    "            \"frame_id\": frame_id,\n",
    "            \"pred_box\": primary_box if has_drone else None,\n",
    "            \"pred_conf\": primary_conf if has_drone else None,\n",
    "            \"gt_box\": gt_box\n",
    "        })\n",
    "\n",
    "        # Update overlay_state for all non-infer frames to reuse\n",
    "        overlay_state.update({\n",
    "            \"last_infer_frame\": frame_id,\n",
    "            \"last_infer_time_s\": t_s,\n",
    "            \"last_mode\": \"GuidedNext\" if primary_source==\"GuidedROI\" else (\"MotionMining\" if do_motion and motion_rois else \"Baseline\"),\n",
    "            \"last_source\": primary_source,\n",
    "            \"last_has_drone\": bool(has_drone),\n",
    "            \"last_conf\": float(primary_conf),\n",
    "            \"last_box\": primary_box,\n",
    "            \"last_verified\": bool(verified),\n",
    "            \"last_verify_mode\": verify_source,\n",
    "            \"last_yolo_calls\": int(infer_calls),\n",
    "            \"motion_rois_used\": len(motion_rois) if do_motion else 0,\n",
    "        })\n",
    "\n",
    "        pre_ms_list.append(pre_ms)\n",
    "        infer_ms_list.append(infer_ms)\n",
    "        post_ms_list.append(post_ms)\n",
    "        if has_drone:\n",
    "            avg_conf_list.append(primary_conf)\n",
    "\n",
    "        # Debug exports\n",
    "        if EXPORT_MOTION_DEBUG_IMAGES and do_motion and motion_keep_mask is not None and motion_dbg_saved < MAX_EXPORT_DEBUG_IMAGES:\n",
    "            dbg = frame.copy()\n",
    "            # draw all motion ROIs (yellow)\n",
    "            for r in motion_rois:\n",
    "                draw_box(dbg, r, (0,255,255), 2, \"Motion ROI (mask bbox)\")\n",
    "            # add ratio\n",
    "            cv2.putText(dbg, f\"motion_ratio={motion_ratio:.3f}\", (10, H-20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,255), 2, cv2.LINE_AA)\n",
    "            outp = os.path.join(MOTION_DEBUG_DIR, f\"{RUN_ID}__infer{frame_id:06d}.jpg\")\n",
    "            cv2.imwrite(outp, dbg)\n",
    "            motion_dbg_saved += 1\n",
    "\n",
    "        if EXPORT_TILED_DEBUG_IMAGES and overlay_state[\"tiles_debug\"] is not None and tiled_dbg_saved < MAX_EXPORT_DEBUG_IMAGES:\n",
    "            dbg = frame.copy()\n",
    "            for r in overlay_state[\"tiles_debug\"]:\n",
    "                draw_box(dbg, r, (255,255,0), 1, None)\n",
    "            outp = os.path.join(TILED_DEBUG_DIR, f\"{RUN_ID}__infer{frame_id:06d}.jpg\")\n",
    "            cv2.imwrite(outp, dbg)\n",
    "            tiled_dbg_saved += 1\n",
    "\n",
    "    # --- Draw overlay for every frame (reuses overlay_state) ---\n",
    "    vis = frame.copy()\n",
    "\n",
    "    # Draw GT box (optional)\n",
    "    gt_box = GT.get(frame_id, None)\n",
    "    if SHOW_GT_RECT and gt_box is not None:\n",
    "        draw_box(vis, gt_box, (0,255,0), 2, \"GT drone\")\n",
    "\n",
    "    # Draw last detection box (red)\n",
    "    if overlay_state[\"last_box\"] is not None and overlay_state[\"last_has_drone\"]:\n",
    "        draw_box(vis, overlay_state[\"last_box\"], (0,0,255), 2, f\"Pred conf={overlay_state['last_conf']:.2f}\")\n",
    "\n",
    "    # Draw guided ROI (cyan) if used on last inference\n",
    "    if overlay_state[\"guided_rect_used\"] is not None:\n",
    "        draw_box(vis, overlay_state[\"guided_rect_used\"], (255,255,0), 2, \"Guided ROI (next inference)\")\n",
    "\n",
    "    # Draw motion ROI crops used (yellow)\n",
    "    for r in overlay_state[\"motion_rois_rects\"]:\n",
    "        draw_box(vis, r, (0,255,255), 2, \"Motion ROI crop\")\n",
    "\n",
    "    # Draw confirmation-guided ROI region (magenta)\n",
    "    if overlay_state[\"confirm_guided_rect\"] is not None:\n",
    "        draw_box(vis, overlay_state[\"confirm_guided_rect\"], (255,0,255), 2, \"Confirmation guided ROI\")\n",
    "\n",
    "    # Continuity debug (optional)\n",
    "    if SHOW_CONTINUITY_DEBUG and overlay_state[\"continuity_debug\"] is not None and overlay_state[\"last_box\"] is not None:\n",
    "        info = overlay_state[\"continuity_debug\"]\n",
    "        if \"prev_exp\" in info:\n",
    "            draw_box(vis, info[\"prev_exp\"], (255,0,0), 2, \"Expanded gate\")\n",
    "        if \"pc\" in info and \"thr\" in info:\n",
    "            pc = info[\"pc\"]; thr = info[\"thr\"]\n",
    "            cv2.circle(vis, (int(pc[0]),int(pc[1])), int(thr), (255,0,0), 2)\n",
    "        if \"pred\" in info and \"gate\" in info:\n",
    "            pred = info[\"pred\"]; gatepx = info[\"gate\"]\n",
    "            cv2.circle(vis, (int(pred[0]),int(pred[1])), int(gatepx), (255,0,0), 2)\n",
    "\n",
    "    # Text overlay\n",
    "    y=22\n",
    "    for line in overlay_lines(frame_id, t_s):\n",
    "        cv2.putText(vis, line, (10,y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2, cv2.LINE_AA)\n",
    "        y += 22\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.write(vis)\n",
    "\n",
    "    if SHOW_PREVIEW:\n",
    "        cv2.imshow(\"benchmark\", vis)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    # per-frame csv row (use overlay_state for stable values)\n",
    "    csv_rows.append([\n",
    "        frame_id, t_s, is_infer,\n",
    "        overlay_state[\"last_mode\"], overlay_state[\"last_source\"],\n",
    "        overlay_state[\"last_has_drone\"], overlay_state[\"last_conf\"], overlay_state[\"last_box\"],\n",
    "        overlay_state[\"last_verified\"], overlay_state[\"last_verify_mode\"], overlay_state[\"last_yolo_calls\"],\n",
    "        gt_box,\n",
    "        sum(1 for e in confirm_hits if e[0]),\n",
    "        sum(1 for e in warning_hits if e[0]),\n",
    "        confirmed_events, warning_events,\n",
    "        no_drone_streak,\n",
    "        counts[\"full_hit\"], counts[\"guided_hit\"], counts[\"motion_hit\"], counts[\"verified_hit\"]\n",
    "    ])\n",
    "\n",
    "    frame_id += 1\n",
    "\n",
    "cap.release()\n",
    "if writer is not None:\n",
    "    writer.release()\n",
    "if SHOW_PREVIEW:\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Save per-frame csv\n",
    "df = pd.DataFrame(csv_rows, columns=csv_cols)\n",
    "df.to_csv(PER_FRAME_CSV, index=False)\n",
    "print(\"Saved per-frame CSV:\", PER_FRAME_CSV)\n",
    "print(\"Saved video:\", OUT_VIDEO_PATH)\n",
    "\n",
    "# Summary metrics (inference frames only)\n",
    "# Precision/Recall at IoU>=0.5\n",
    "tp=fp=fn=0\n",
    "for r in infer_eval:\n",
    "    gb = r[\"gt_box\"]\n",
    "    pb = r[\"pred_box\"]\n",
    "    if gb is None and pb is None:\n",
    "        continue\n",
    "    if gb is None and pb is not None:\n",
    "        fp += 1\n",
    "    elif gb is not None and pb is None:\n",
    "        fn += 1\n",
    "    else:\n",
    "        if iou_xyxy(pb, gb) >= 0.5:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "precision = tp / max(1, tp+fp)\n",
    "recall = tp / max(1, tp+fn)\n",
    "ap50 = compute_ap50(infer_eval)\n",
    "\n",
    "summary = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"video\": os.path.basename(VIDEO_PATH),\n",
    "    \"infer_fps\": INFER_FPS,\n",
    "    \"pre_motion_method\": PRE_MOTION_METHOD,\n",
    "    \"use_motion\": USE_MOTION_MINING,\n",
    "    \"use_guided_next\": USE_GUIDED_ROI_NEXT_INFER,\n",
    "    \"verify_mode\": VERIFY_MODE,\n",
    "    \"confirm_window\": CONFIRM_WINDOW,\n",
    "    \"confirm_require\": CONFIRM_REQUIRE,\n",
    "    \"warning_window\": WARNING_WINDOW,\n",
    "    \"warning_require\": WARNING_REQUIRE,\n",
    "    \"base_conf_full\": BASE_CONF_FULL,\n",
    "    \"guided_roi_conf\": GUIDED_ROI_CONF,\n",
    "    \"low_conf_detection\": LOW_CONF_DETECTION,\n",
    "    \"precision_infer\": precision,\n",
    "    \"recall_infer\": recall,\n",
    "    \"ap50_infer\": ap50,\n",
    "    \"avg_conf_infer\": safe_mean(avg_conf_list),\n",
    "    \"avg_pre_ms\": safe_mean(pre_ms_list),\n",
    "    \"avg_infer_ms\": safe_mean(infer_ms_list),\n",
    "    \"avg_post_ms\": safe_mean(post_ms_list),\n",
    "    \"confirmed_events\": confirmed_events,\n",
    "    \"warning_events\": warning_events,\n",
    "}\n",
    "\n",
    "print(\"\\n=== SUMMARY (inference frames only) ===\")\n",
    "for k,v in summary.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# Append to master summary CSV\n",
    "with open(MASTER_SUMMARY_CSV, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\n",
    "        summary[\"run_id\"], summary[\"video\"], summary[\"infer_fps\"], summary[\"pre_motion_method\"],\n",
    "        summary[\"use_motion\"], summary[\"use_guided_next\"], summary[\"verify_mode\"],\n",
    "        summary[\"confirm_window\"], summary[\"confirm_require\"], summary[\"warning_window\"], summary[\"warning_require\"],\n",
    "        summary[\"base_conf_full\"], summary[\"guided_roi_conf\"], summary[\"low_conf_detection\"],\n",
    "        summary[\"precision_infer\"], summary[\"recall_infer\"], summary[\"ap50_infer\"],\n",
    "        summary[\"avg_conf_infer\"],\n",
    "        summary[\"avg_pre_ms\"], summary[\"avg_infer_ms\"], summary[\"avg_post_ms\"],\n",
    "        summary[\"confirmed_events\"], summary[\"warning_events\"],\n",
    "        \"\"\n",
    "    ])\n",
    "print(\"\\nAppended run summary to:\", MASTER_SUMMARY_CSV)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video FPS: 29.970933447224024 Frames: 1351 Infer stride: 6\n",
      "Saved per-frame CSV: video_benchmark_outputs\\gopro_006__infer5fps__motion-None__verify-None__per_frame_log.csv\n",
      "Saved video: video_benchmark_outputs\\gopro_006__infer5fps__motion-None__verify-None.mp4\n",
      "\n",
      "=== SUMMARY (inference frames only) ===\n",
      "run_id: gopro_006__infer5fps__motion-None__verify-None\n",
      "video: gopro_006.mp4\n",
      "infer_fps: 5\n",
      "pre_motion_method: None\n",
      "use_motion: True\n",
      "use_guided_next: True\n",
      "verify_mode: None\n",
      "confirm_window: 10\n",
      "confirm_require: 9\n",
      "warning_window: 10\n",
      "warning_require: 9\n",
      "base_conf_full: 0.25\n",
      "guided_roi_conf: 0.4\n",
      "low_conf_detection: 0.4\n",
      "precision_infer: 0.3404255319148936\n",
      "recall_infer: 0.2909090909090909\n",
      "ap50_infer: 0.1824841824841825\n",
      "avg_conf_infer: 0.6580467341428108\n",
      "avg_pre_ms: 2.349524335977951\n",
      "avg_infer_ms: 74.47989778714233\n",
      "avg_post_ms: 28.52686017762511\n",
      "confirmed_events: 1\n",
      "warning_events: 0\n",
      "\n",
      "Appended run summary to: video_benchmark_outputs\\benchmark_runs_summary.csv\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "ed88ca45",
   "metadata": {},
   "source": [
    "## 10) How many frames should motion ROI mining buffer?\n",
    "In this notebook, motion mining buffers **inference frames** (`MOTION_BUFFER_STEPS`), not all video frames.\n",
    "\n",
    "Practical guidance:\n",
    "- **Static camera**: 3–7 inference frames is typically enough.\n",
    "- **Trees/leaves**: increasing buffer helps only if you also use **persistence** (`MOTION_PERSIST_MIN_HITS`) so that random flicker is rejected.\n",
    "- **Too many frames** can dilute fast targets (the drone moves, masks stop overlapping), so the persistence threshold must be tuned together with buffer length.\n",
    "\n",
    "If `INFER_FPS=5`, buffering 5 inference frames covers ~1 second of time. For fast drones, you may prefer `MOTION_BUFFER_STEPS=3` with `MOTION_PERSIST_MIN_HITS=2`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
