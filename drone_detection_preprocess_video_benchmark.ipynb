{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b73833",
   "metadata": {},
   "source": [
    "# Drone detection — moving-camera preprocessing + video benchmark (YOLO11)\n",
    "\n",
    "This notebook continues your existing project state and focuses on **preprocessing experiments** that remain valid under:\n",
    "\n",
    "- moving cameras (no static background assumptions)\n",
    "- no hard-coded ROIs\n",
    "- realistic CCTV-like clutter\n",
    "\n",
    "You asked for:\n",
    "\n",
    "- **imgsz kept at 640** (trained at 640)\n",
    "- **video input inference** (not folder-based images)\n",
    "- ability to run **each method individually** and visually inspect detections in (near) real time\n",
    "- **logging** per method to compare recall/precision (when annotations exist) and FPS\n",
    "- optional **ROI-zoom** preprocessing to help tiny drones without retraining\n",
    "\n",
    "Notebook structure is strictly:\n",
    "\n",
    "1) Pre-processing\n",
    "2) YOLO11 inference\n",
    "3) Post-processing + evaluation\n",
    "\n",
    "---\n",
    "\n",
    "## Why earlier you saw 384\n",
    "Your footage often looks like wide frames (for example, 16:9). A common fast trick is to run at 640×384 to match aspect ratio and reduce padding.\n",
    "\n",
    "But **you trained at imgsz=640**, so here we keep **imgsz=640** and let Ultralytics letterbox as needed.\n",
    "\n",
    "## Why not only `yolo` CLI commands\n",
    "The `yolo detect predict` CLI is great for baseline runs, but it does **not** easily let you:\n",
    "\n",
    "- inject custom preprocessing per frame\n",
    "- do ROI-zoom (crop → re-infer → map boxes back)\n",
    "- collect per-frame logs and optional GT evaluation\n",
    "\n",
    "So this notebook uses the Ultralytics **Python API** for flexibility, and also includes CLI cells for reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "127af3cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T13:00:08.951250Z",
     "start_time": "2026-01-16T13:00:08.943198Z"
    }
   },
   "source": [
    "# Cell 1 — Imports\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import torch\n",
    "import ultralytics\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Ultralytics: {ultralytics.__version__}\")"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "7dc36ffd",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your **model weights** and **video path** here.\n",
    "\n",
    "Important: paths are not guessed. If a path is empty or invalid, the notebook will stop with a clear error.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "83639f69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T13:00:14.945940Z",
     "start_time": "2026-01-16T13:00:14.930300Z"
    }
   },
   "source": [
    "# Cell 2 — Config (edit these)\n",
    "\n",
    "# --- REQUIRED ---\n",
    "MODEL_WEIGHTS = r\"S:\\IntelliJ\\Projects\\ES_Drone_Detection\\runs\\detect\\yolo11\\drone_finetune_full_mixed4\\weights\\best.pt\"\n",
    "VIDEO_PATH    = r\"S:\\IntelliJ\\Projects\\ES_Drone_Detection\\video_test\\GOPR5844_002.mp4\"\n",
    "\n",
    "# If you place a text annotation next to the video with the same stem (e.g., swarm_dji_phantom.txt),\n",
    "# the notebook will auto-load it for GT evaluation.\n",
    "# You can also set this explicitly.\n",
    "VIDEO_ANN_PATH = r\"\"  # optional; leave empty to auto-detect VIDEO_PATH with .txt\n",
    "\n",
    "# --- Output ---\n",
    "OUT_DIR = Path(\"video_benchmark_outputs\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- YOLO inference params ---\n",
    "IMGSZ = 640           # keep 640 (trained at 640)\n",
    "CONF = 0.25\n",
    "IOU_NMS = 0.7\n",
    "DEVICE = 0            # 0 for GPU, 'cpu' for CPU\n",
    "HALF = False           # FP16 on GPU => faster\n",
    "MAX_DET = 30         # cap detections per frame (speed + stability)\n",
    "\n",
    "# --- Display ---\n",
    "# 'window' => cv2.imshow (best for local runs)\n",
    "# 'inline' => renders frames inline (Colab-friendly, slower)\n",
    "DISPLAY_MODE = 'window'\n",
    "DISPLAY_MAX_FPS = 60  # throttles UI refresh only (not inference)\n",
    "\n",
    "# --- Logging ---\n",
    "LOG_EVERY_N_FRAMES = 5\n",
    "\n",
    "# --- Ground-truth evaluation ---\n",
    "# Supports either:\n",
    "#   A) VIDEO_ANN_PATH in WOSDETC-like format: frame_id num_objs (x y w h label)+\n",
    "#   B) Standard YOLO frame labels via extracted frames (optional; not required)\n",
    "IOU_MATCH = 0.5\n",
    "\n",
    "GT_FRAMES_DIR = r\"\"  # optional (standard YOLO frames)\n",
    "GT_LABELS_DIR = r\"\"  # optional (standard YOLO labels)\n",
    "\n",
    "# --- Optional: ROI-zoom (crop-based) ---\n",
    "# WARNING: any crop-based zoom that uses YOLO to propose ROIs adds extra inference.\n",
    "# To reduce the speed hit, you can make it CONDITIONAL.\n",
    "ROI_MODE = \"off\"        # 'off' | 'conditional' | 'always'\n",
    "ROI_COARSE_CONF = 0.08   # lower conf to get candidates for ROI proposal\n",
    "ROI_PAD_FRAC = 0.40      # expand crop by this fraction around each box\n",
    "ROI_MAX_CROPS = 4        # cap crops per frame\n",
    "ROI_MIN_BOX_AREA_FRAC = 0.00002  # ignore ultra-tiny coarse boxes (often noise)\n",
    "ROI_MERGE_IOU = 0.55     # NMS when merging full-frame + crop detections\n",
    "\n",
    "# Conditional ROI triggers (only used when ROI_MODE='conditional')\n",
    "ROI_TRIGGER_IF_NO_DETS = True\n",
    "ROI_TRIGGER_MAX_FULLFRAME_DETS = 0  # if >0, trigger ROI when full-frame det count <= this\n",
    "ROI_TRIGGER_IF_SMALL = True\n",
    "ROI_TRIGGER_SMALL_AREA_FRAC = 0.00008  # trigger ROI if smallest det area <= this\n",
    "\n",
    "\n",
    "def _require_path(p, name):\n",
    "    if not p or str(p).strip() == \"\":\n",
    "        raise ValueError(f\"{name} is empty. Set it to a valid path.\")\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"{name} does not exist: {p}\")\n",
    "\n",
    "_require_path(MODEL_WEIGHTS, \"MODEL_WEIGHTS\")\n",
    "_require_path(VIDEO_PATH, \"VIDEO_PATH\")\n",
    "\n",
    "# Auto-detect sidecar annotation if not explicitly set\n",
    "if not VIDEO_ANN_PATH:\n",
    "    sidecar = str(Path(VIDEO_PATH).with_suffix('.txt'))\n",
    "    if os.path.exists(sidecar):\n",
    "        VIDEO_ANN_PATH = sidecar\n",
    "\n",
    "print(\"OK: config loaded\")\n",
    "print(\"MODEL_WEIGHTS:\", MODEL_WEIGHTS)\n",
    "print(\"VIDEO_PATH:\", VIDEO_PATH)\n",
    "print(\"VIDEO_ANN_PATH:\", VIDEO_ANN_PATH if VIDEO_ANN_PATH else \"(none)\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: config loaded\n",
      "MODEL_WEIGHTS: S:\\IntelliJ\\Projects\\ES_Drone_Detection\\runs\\detect\\yolo11\\drone_finetune_full_mixed4\\weights\\best.pt\n",
      "VIDEO_PATH: S:\\IntelliJ\\Projects\\ES_Drone_Detection\\video_test\\GOPR5844_002.mp4\n",
      "VIDEO_ANN_PATH: S:\\IntelliJ\\Projects\\ES_Drone_Detection\\video_test\\GOPR5844_002.txt\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "2089b3cb",
   "metadata": {},
   "source": [
    "## Pre-processing methods (moving-camera safe)\n",
    "\n",
    "All methods below are **per-frame** (no background model, no ROI assumptions), so they stay valid with strong camera motion.\n",
    "\n",
    "You can run any method alone, or combine it with ROI-zoom.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "9fb8378b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T13:00:18.143360Z",
     "start_time": "2026-01-16T13:00:18.125772Z"
    }
   },
   "source": [
    "# Cell 3 — Pre-processing functions\n",
    "\n",
    "def to_ycrcb_clahe(bgr, clip_limit=2.0, tile_grid=(8, 8)):\n",
    "    \"\"\"CLAHE on luminance (Y) only: boosts contrast without color artifacts.\"\"\"\n",
    "    ycrcb = cv2.cvtColor(bgr, cv2.COLOR_BGR2YCrCb)\n",
    "    y, cr, cb = cv2.split(ycrcb)\n",
    "    clahe = cv2.createCLAHE(clipLimit=float(clip_limit), tileGridSize=tuple(tile_grid))\n",
    "    y2 = clahe.apply(y)\n",
    "    out = cv2.merge([y2, cr, cb])\n",
    "    return cv2.cvtColor(out, cv2.COLOR_YCrCb2BGR)\n",
    "\n",
    "\n",
    "def gray_world_wb(bgr):\n",
    "    \"\"\"Simple gray-world white balance: reduces camera color cast.\"\"\"\n",
    "    b, g, r = cv2.split(bgr.astype(np.float32))\n",
    "    mb, mg, mr = b.mean(), g.mean(), r.mean()\n",
    "    m = (mb + mg + mr) / 3.0\n",
    "    b *= (m / (mb + 1e-6))\n",
    "    g *= (m / (mg + 1e-6))\n",
    "    r *= (m / (mr + 1e-6))\n",
    "    out = cv2.merge([b, g, r])\n",
    "    return np.clip(out, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def unsharp_mask(bgr, amount=0.5, blur_ksize=3):\n",
    "    \"\"\"Mild edge boost. Keep amount low to avoid false positives.\"\"\"\n",
    "    k = int(blur_ksize)\n",
    "    if k % 2 == 0:\n",
    "        k += 1\n",
    "    blur = cv2.GaussianBlur(bgr, (k, k), 0)\n",
    "    sharp = cv2.addWeighted(bgr, 1.0 + float(amount), blur, -float(amount), 0)\n",
    "    return sharp\n",
    "\n",
    "\n",
    "def light_bilateral(bgr, d=5, sigma_color=35, sigma_space=35):\n",
    "    \"\"\"Edge-preserving denoise. Use small parameters for speed.\"\"\"\n",
    "    return cv2.bilateralFilter(bgr, int(d), float(sigma_color), float(sigma_space))\n",
    "\n",
    "\n",
    "def preprocess_pipeline(name, bgr):\n",
    "    if name == \"baseline\":\n",
    "        return bgr\n",
    "\n",
    "    if name == \"clahe_y\":\n",
    "        return to_ycrcb_clahe(bgr)\n",
    "\n",
    "    if name == \"clahe_y + unsharp\":\n",
    "        x = to_ycrcb_clahe(bgr)\n",
    "        return unsharp_mask(x, amount=0.5, blur_ksize=3)\n",
    "\n",
    "    if name == \"wb + clahe_y\":\n",
    "        x = gray_world_wb(bgr)\n",
    "        return to_ycrcb_clahe(x)\n",
    "\n",
    "    if name == \"clahe_y + bilateral + unsharp\":\n",
    "        x = to_ycrcb_clahe(bgr)\n",
    "        x = light_bilateral(x)\n",
    "        return unsharp_mask(x, amount=0.4, blur_ksize=3)\n",
    "\n",
    "    raise ValueError(f\"Unknown pipeline: {name} Valid: baseline, clahe_y, clahe_y + unsharp, wb + clahe_y, clahe_y + bilateral + unsharp\")\n",
    "\n",
    "PIPELINES = [\n",
    "    \"baseline\",\n",
    "    \"clahe_y\",\n",
    "    \"clahe_y + unsharp\",\n",
    "    \"wb + clahe_y\",\n",
    "    \"clahe_y + bilateral + unsharp\",\n",
    "]\n",
    "\n",
    "print(\"Available pipelines:\", PIPELINES)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available pipelines: ['baseline', 'clahe_y', 'clahe_y + unsharp', 'wb + clahe_y', 'clahe_y + bilateral + unsharp']\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "d6c83fc8",
   "metadata": {},
   "source": [
    "## YOLO11 model load\n",
    "\n",
    "Loads your trained YOLO11n weights. We keep **imgsz=640** for predict.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "39dedaae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T13:00:31.382044Z",
     "start_time": "2026-01-16T13:00:31.334629Z"
    }
   },
   "source": [
    "# Cell 4 — Load model\n",
    "model = YOLO(MODEL_WEIGHTS)\n",
    "print(\"Model loaded:\", MODEL_WEIGHTS)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: S:\\IntelliJ\\Projects\\ES_Drone_Detection\\runs\\detect\\yolo11\\drone_finetune_full_mixed4\\weights\\best.pt\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "5cdecc32",
   "metadata": {},
   "source": [
    "## Baseline CLI note\n",
    "\n",
    "Ultralytics CLI is useful for **baseline** runs, but it needs real model and video paths.\n",
    "\n",
    "This notebook uses the **Python API** because it gives per-frame hooks for preprocessing, optional ROI-zoom, visualization, and logging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf04d9e",
   "metadata": {},
   "source": [
    "## Post-processing and optional ROI-zoom\n",
    "\n",
    "### ROI-zoom (crop-based) idea\n",
    "\n",
    "When drones are tiny, letterboxing into 640 can shrink them further. ROI-zoom tries to help without retraining:\n",
    "\n",
    "1) **Coarse pass** on full frame at low conf to propose candidate regions.\n",
    "2) Crop around candidates (with padding), resize each crop to 640, run YOLO again.\n",
    "3) Map crop detections back to original frame and merge with NMS.\n",
    "\n",
    "This does **not** assume static background or fixed ROIs. It does cost more compute, so you benchmark FPS.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6b4bdef2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T13:04:26.481609Z",
     "start_time": "2026-01-16T13:04:26.450750Z"
    }
   },
   "source": [
    "# Cell 6 — Helpers: IoU + NMS + ROI crops\n",
    "\n",
    "\n",
    "def box_iou_xyxy(a, b):\n",
    "    ax1, ay1, ax2, ay2 = a\n",
    "    bx1, by1, bx2, by2 = b\n",
    "    ix1 = max(ax1, bx1)\n",
    "    iy1 = max(ay1, by1)\n",
    "    ix2 = min(ax2, bx2)\n",
    "    iy2 = min(ay2, by2)\n",
    "    iw = max(0.0, ix2 - ix1)\n",
    "    ih = max(0.0, iy2 - iy1)\n",
    "    inter = iw * ih\n",
    "    area_a = max(0.0, ax2-ax1) * max(0.0, ay2-ay1)\n",
    "    area_b = max(0.0, bx2-bx1) * max(0.0, by2-by1)\n",
    "    union = area_a + area_b - inter + 1e-9\n",
    "    return inter / union\n",
    "\n",
    "\n",
    "def nms_xyxy_merge(dets, iou_thr=0.55):\n",
    "    \"\"\"dets: list of (x1,y1,x2,y2,conf,cls). returns filtered list.\"\"\"\n",
    "    if not dets:\n",
    "        return []\n",
    "    dets = sorted(dets, key=lambda d: d[4], reverse=True)\n",
    "    keep = []\n",
    "    used = [False]*len(dets)\n",
    "    for i, di in enumerate(dets):\n",
    "        if used[i]:\n",
    "            continue\n",
    "        keep.append(di)\n",
    "        xi1, yi1, xi2, yi2, ci, cls_i = di\n",
    "        for j in range(i+1, len(dets)):\n",
    "            if used[j]:\n",
    "                continue\n",
    "            dj = dets[j]\n",
    "            if int(dj[5]) != int(cls_i):\n",
    "                continue\n",
    "            iou = box_iou_xyxy((xi1,yi1,xi2,yi2), (dj[0],dj[1],dj[2],dj[3]))\n",
    "            if iou >= float(iou_thr):\n",
    "                used[j] = True\n",
    "    return keep\n",
    "\n",
    "\n",
    "def build_crops_from_dets(dets, img_w, img_h, pad_frac=0.4, max_crops=6, min_area_frac=0.00002):\n",
    "    \"\"\"Create crop rectangles (x1,y1,x2,y2) from detections.\n",
    "    dets: (x1,y1,x2,y2,conf,cls)\n",
    "    \"\"\"\n",
    "    if not dets:\n",
    "        return []\n",
    "    denom = float(img_w * img_h) + 1e-9\n",
    "    # sort by confidence and keep top candidates\n",
    "    dets = sorted(dets, key=lambda d: d[4], reverse=True)\n",
    "\n",
    "    crops = []\n",
    "    for d in dets:\n",
    "        x1,y1,x2,y2,conf,cls = d\n",
    "        area_frac = ((x2-x1)*(y2-y1))/denom\n",
    "        if area_frac < float(min_area_frac):\n",
    "            continue\n",
    "        bw = (x2-x1)\n",
    "        bh = (y2-y1)\n",
    "        pad_x = bw * float(pad_frac)\n",
    "        pad_y = bh * float(pad_frac)\n",
    "        cx1 = max(0.0, x1 - pad_x)\n",
    "        cy1 = max(0.0, y1 - pad_y)\n",
    "        cx2 = min(float(img_w), x2 + pad_x)\n",
    "        cy2 = min(float(img_h), y2 + pad_y)\n",
    "        # Avoid degenerate crops\n",
    "        if (cx2 - cx1) < 10 or (cy2 - cy1) < 10:\n",
    "            continue\n",
    "        crops.append((cx1, cy1, cx2, cy2))\n",
    "        if len(crops) >= int(max_crops):\n",
    "            break\n",
    "\n",
    "    # Merge overlapping crops (simple)\n",
    "    merged = []\n",
    "    for c in crops:\n",
    "        added = False\n",
    "        for k in range(len(merged)):\n",
    "            m = merged[k]\n",
    "            iou = box_iou_xyxy(m, c)\n",
    "            if iou >= 0.2:\n",
    "                merged[k] = (\n",
    "                    min(m[0], c[0]),\n",
    "                    min(m[1], c[1]),\n",
    "                    max(m[2], c[2]),\n",
    "                    max(m[3], c[3]),\n",
    "                )\n",
    "                added = True\n",
    "                break\n",
    "        if not added:\n",
    "            merged.append(c)\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "def map_crop_dets_to_full(dets_crop, crop_x1, crop_y1):\n",
    "    \"\"\"Map detections from crop coords back to full-frame coords.\"\"\"\n",
    "    out = []\n",
    "    for x1,y1,x2,y2,conf,cls in dets_crop:\n",
    "        out.append((x1+crop_x1, y1+crop_y1, x2+crop_x1, y2+crop_y1, conf, cls))\n",
    "    return out\n",
    "\n",
    "\n",
    "def match_detections_to_gt(pred_boxes, gt_boxes, iou_thr=0.5):\n",
    "    \"\"\"pred_boxes: list (cls, conf, x1,y1,x2,y2)\n",
    "       gt_boxes: list (cls, x1,y1,x2,y2)\n",
    "    \"\"\"\n",
    "    gt_used = [False]*len(gt_boxes)\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    ious = []\n",
    "\n",
    "    pred_boxes = sorted(pred_boxes, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    for p in pred_boxes:\n",
    "        pcls, conf, px1, py1, px2, py2 = p\n",
    "        best_iou = 0.0\n",
    "        best_j = -1\n",
    "        for j, g in enumerate(gt_boxes):\n",
    "            if gt_used[j]:\n",
    "                continue\n",
    "            gcls, gx1, gy1, gx2, gy2 = g\n",
    "            if int(pcls) != int(gcls):\n",
    "                continue\n",
    "            iou = box_iou_xyxy((px1,py1,px2,py2), (gx1,gy1,gx2,gy2))\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_j = j\n",
    "        if best_iou >= float(iou_thr) and best_j >= 0:\n",
    "            gt_used[best_j] = True\n",
    "            tp += 1\n",
    "            ious.append(best_iou)\n",
    "        else:\n",
    "            fp += 1\n",
    "\n",
    "    fn = sum(1 for u in gt_used if not u)\n",
    "    return tp, fp, fn, ious\n"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "9d0c5c58",
   "metadata": {},
   "source": [
    "## Optional evaluation (if you prepared YOLO labels per extracted frame)\n",
    "\n",
    "If `GT_FRAMES_DIR` and `GT_LABELS_DIR` are set, the notebook will score:\n",
    "\n",
    "- TP / FP / FN per frame at IoU = `IOU_MATCH`\n",
    "- precision / recall / F1 over the video segment\n",
    "\n",
    "If not set, it will still log detection stats + FPS.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e6e2b2bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T13:04:47.245350Z",
     "start_time": "2026-01-16T13:04:47.229705Z"
    }
   },
   "source": [
    "# Cell 7 — Ground truth loaders (optional)\n",
    "\n",
    "# 1) Sidecar video annotations (recommended for your current benchmark)\n",
    "# Format (WOSDETC-like):\n",
    "#   frame_id num_objs  x y w h label  x y w h label ...\n",
    "# Example:\n",
    "#   0 2 233 137 16 14 drone 450 248 11 12 drone\n",
    "\n",
    "\n",
    "def load_sidecar_annotations(txt_path, class_names=None):\n",
    "    \"\"\"Returns dict[int frame_idx] -> list[(cls, x1,y1,x2,y2)] in PIXELS.\"\"\"\n",
    "    if not txt_path or not os.path.exists(txt_path):\n",
    "        return {}\n",
    "\n",
    "    # Map string labels to class indices.\n",
    "    # If model is single-class, everything becomes 0.\n",
    "    name_to_idx = None\n",
    "    if class_names and isinstance(class_names, dict):\n",
    "        # ultralytics may store dict idx->name\n",
    "        name_to_idx = {v: int(k) for k, v in class_names.items()}\n",
    "    elif class_names and isinstance(class_names, (list, tuple)):\n",
    "        name_to_idx = {str(v): i for i, v in enumerate(class_names)}\n",
    "\n",
    "    out = {}\n",
    "    with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            # frame_id num_objs then 5*num_objs tokens\n",
    "            frame_id = int(parts[0])\n",
    "            n = int(parts[1])\n",
    "            expect = 2 + 5*n\n",
    "            if len(parts) < expect:\n",
    "                # skip malformed lines\n",
    "                continue\n",
    "            boxes = []\n",
    "            j = 2\n",
    "            for _ in range(n):\n",
    "                x = float(parts[j]); y = float(parts[j+1]); w = float(parts[j+2]); h = float(parts[j+3]); lab = parts[j+4]\n",
    "                j += 5\n",
    "                # label -> cls\n",
    "                if name_to_idx is None:\n",
    "                    cls = 0\n",
    "                else:\n",
    "                    cls = name_to_idx.get(lab, 0)\n",
    "                x1 = x\n",
    "                y1 = y\n",
    "                x2 = x + w\n",
    "                y2 = y + h\n",
    "                boxes.append((cls, x1, y1, x2, y2))\n",
    "            out[frame_id] = boxes\n",
    "    return out\n",
    "\n",
    "\n",
    "# 2) Standard YOLO per-frame labels (optional)\n",
    "\n",
    "def _gt_yolo_enabled():\n",
    "    return bool(GT_FRAMES_DIR and GT_LABELS_DIR and os.path.exists(GT_LABELS_DIR))\n",
    "\n",
    "\n",
    "def yolo_txt_to_boxes(label_txt_path, img_w, img_h):\n",
    "    # returns list of (cls, x1,y1,x2,y2)\n",
    "    if not os.path.exists(label_txt_path):\n",
    "        return []\n",
    "    out = []\n",
    "    with open(label_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            cls, xc, yc, w, h = line.split()\n",
    "            cls = int(float(cls))\n",
    "            xc, yc, w, h = map(float, (xc, yc, w, h))\n",
    "            x1 = (xc - w/2) * img_w\n",
    "            y1 = (yc - h/2) * img_h\n",
    "            x2 = (xc + w/2) * img_w\n",
    "            y2 = (yc + h/2) * img_h\n",
    "            out.append((cls, x1, y1, x2, y2))\n",
    "    return out\n",
    "\n",
    "\n",
    "def frame_idx_to_yolo_label_path(frame_idx):\n",
    "    # expects frame_000001.jpg -> frame_000001.txt\n",
    "    stem = f\"frame_{frame_idx:06d}.txt\"\n",
    "    return str(Path(GT_LABELS_DIR) / stem)\n"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "b29e4bf8",
   "metadata": {},
   "source": [
    "## Video inference runner (one method at a time)\n",
    "\n",
    "This is the main loop:\n",
    "\n",
    "- reads video frames\n",
    "- applies the selected preprocessing\n",
    "- runs YOLO inference\n",
    "- optional ROI-zoom\n",
    "- draws boxes + FPS overlay\n",
    "- logs per-frame stats and optional GT scores\n",
    "\n",
    "Outputs:\n",
    "\n",
    "- annotated MP4\n",
    "- CSV log per run\n",
    "- one-line summary metrics printed at the end\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "0ddf16c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T13:04:59.608639Z",
     "start_time": "2026-01-16T13:04:59.579896Z"
    }
   },
   "source": [
    "# Cell 8 — Video benchmark runner\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "def draw_dets(frame_bgr, dets, class_names=None):\n",
    "    out = frame_bgr.copy()\n",
    "    for x1, y1, x2, y2, conf, cls in dets:\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "        cls_i = int(cls)\n",
    "        label = f\"{cls_i}:{conf:.2f}\" if not class_names else f\"{class_names.get(cls_i, cls_i)}:{conf:.2f}\"\n",
    "        cv2.rectangle(out, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(out, label, (x1, max(0, y1-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _inline_show(bgr):\n",
    "    # Inline display for Colab/Jupyter. Slower.\n",
    "    from IPython.display import display, clear_output\n",
    "    import matplotlib.pyplot as plt\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.imshow(rgb)\n",
    "    plt.axis('off')\n",
    "    clear_output(wait=True)\n",
    "    display(plt.gcf())\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def _predict_on_bgr(bgr, conf, iou):\n",
    "    # Ultralytics accepts numpy arrays. Keep calls centralized.\n",
    "    res = model.predict(\n",
    "        source=bgr,\n",
    "        imgsz=IMGSZ,\n",
    "        conf=conf,\n",
    "        iou=iou,\n",
    "        device=DEVICE,\n",
    "        half=HALF,\n",
    "        max_det=MAX_DET,\n",
    "        verbose=False,\n",
    "    )[0]\n",
    "\n",
    "    dets = []\n",
    "    if res.boxes is None or len(res.boxes) == 0:\n",
    "        return dets\n",
    "\n",
    "    xyxy = res.boxes.xyxy.cpu().numpy()\n",
    "    confs = res.boxes.conf.cpu().numpy()\n",
    "    clss = res.boxes.cls.cpu().numpy()\n",
    "    for (x1, y1, x2, y2), c, k in zip(xyxy, confs, clss):\n",
    "        dets.append((float(x1), float(y1), float(x2), float(y2), float(c), float(k)))\n",
    "    return dets\n",
    "\n",
    "\n",
    "def _dets_area_frac(dets, img_w, img_h):\n",
    "    if not dets:\n",
    "        return []\n",
    "    a = []\n",
    "    denom = float(img_w * img_h) + 1e-9\n",
    "    for x1,y1,x2,y2,conf,cls in dets:\n",
    "        a.append(((x2-x1)*(y2-y1))/denom)\n",
    "    return a\n",
    "\n",
    "\n",
    "def run_video(method_name, max_frames=None, start_frame=0, save_video=True):\n",
    "    if method_name not in PIPELINES:\n",
    "        raise ValueError(f\"Unknown method: {method_name}. Valid: {PIPELINES}\")\n",
    "\n",
    "    cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Failed to open video: {VIDEO_PATH}\")\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
    "    fps_src = float(cap.get(cv2.CAP_PROP_FPS) or 0.0)\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 0)\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 0)\n",
    "\n",
    "    # Seek\n",
    "    if start_frame > 0:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(start_frame))\n",
    "\n",
    "    # Output paths\n",
    "    stamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "    roi_tag = ROI_MODE\n",
    "    tag = f\"{Path(VIDEO_PATH).stem}__{method_name.replace(' ', '_').replace('+', 'plus')}__roi{roi_tag}__{stamp}\"\n",
    "    out_video_path = OUT_DIR / f\"{tag}.mp4\"\n",
    "    out_frame_csv = OUT_DIR / f\"{tag}__frame_log.csv\"\n",
    "    out_summary_csv = OUT_DIR / f\"{tag}__summary.csv\"\n",
    "\n",
    "    writer = None\n",
    "\n",
    "    # For smooth FPS reporting\n",
    "    t_hist = deque(maxlen=30)\n",
    "    ui_hist = deque(maxlen=10)\n",
    "\n",
    "    # Class names\n",
    "    try:\n",
    "        class_names = model.names  # usually dict\n",
    "    except Exception:\n",
    "        class_names = None\n",
    "\n",
    "    # Sidecar GT\n",
    "    sidecar_gt = load_sidecar_annotations(VIDEO_ANN_PATH, class_names=class_names)\n",
    "\n",
    "    # Metrics accumulators\n",
    "    tot_tp = tot_fp = tot_fn = 0\n",
    "    tot_frames_scored = 0\n",
    "\n",
    "    frame_logs = []\n",
    "\n",
    "    # Video writer\n",
    "    if save_video:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        writer = cv2.VideoWriter(str(out_video_path), fourcc, fps_src if fps_src > 0 else 30.0, (w, h))\n",
    "\n",
    "    # UI pacing\n",
    "    last_ui_t = 0.0\n",
    "\n",
    "    frame_idx = int(cap.get(cv2.CAP_PROP_POS_FRAMES) or start_frame)\n",
    "    processed = 0\n",
    "\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "\n",
    "        # frame_idx is current frame number *before* incrementing\n",
    "        this_frame_idx = frame_idx\n",
    "        frame_idx += 1\n",
    "\n",
    "        if max_frames is not None and processed >= int(max_frames):\n",
    "            break\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "        # 1) preprocess per-frame (moving-camera-safe)\n",
    "        pre = preprocess_pipeline(method_name, frame)\n",
    "\n",
    "        # 2) full-frame inference\n",
    "        dets_full = _predict_on_bgr(pre, conf=CONF, iou=IOU_NMS)\n",
    "\n",
    "        # Decide whether to trigger ROI zoom\n",
    "        do_roi = False\n",
    "        if ROI_MODE == 'always':\n",
    "            do_roi = True\n",
    "        elif ROI_MODE == 'conditional':\n",
    "            if ROI_TRIGGER_IF_NO_DETS and len(dets_full) == 0:\n",
    "                do_roi = True\n",
    "            if ROI_TRIGGER_MAX_FULLFRAME_DETS > 0 and len(dets_full) <= ROI_TRIGGER_MAX_FULLFRAME_DETS:\n",
    "                do_roi = True\n",
    "            if ROI_TRIGGER_IF_SMALL and len(dets_full) > 0:\n",
    "                areas = _dets_area_frac(dets_full, w, h)\n",
    "                if areas and min(areas) <= ROI_TRIGGER_SMALL_AREA_FRAC:\n",
    "                    do_roi = True\n",
    "\n",
    "        dets = dets_full\n",
    "\n",
    "        # 3) optional ROI zoom\n",
    "        if do_roi and ROI_MODE != 'off':\n",
    "            # coarse pass uses SAME model but lower conf to propose ROIs\n",
    "            dets_coarse = _predict_on_bgr(pre, conf=ROI_COARSE_CONF, iou=IOU_NMS)\n",
    "\n",
    "            # Build crops from coarse boxes\n",
    "            crops = build_crops_from_dets(\n",
    "                dets_coarse,\n",
    "                img_w=w,\n",
    "                img_h=h,\n",
    "                pad_frac=ROI_PAD_FRAC,\n",
    "                max_crops=ROI_MAX_CROPS,\n",
    "                min_area_frac=ROI_MIN_BOX_AREA_FRAC,\n",
    "            )\n",
    "\n",
    "            dets_zoom_all = []\n",
    "            for (cx1, cy1, cx2, cy2) in crops:\n",
    "                crop = pre[int(cy1):int(cy2), int(cx1):int(cx2)]\n",
    "                if crop.size == 0:\n",
    "                    continue\n",
    "                dets_crop = _predict_on_bgr(crop, conf=CONF, iou=IOU_NMS)\n",
    "                # Map crop dets back to full-frame coords\n",
    "                dets_mapped = map_crop_dets_to_full(dets_crop, cx1, cy1)\n",
    "                dets_zoom_all.extend(dets_mapped)\n",
    "\n",
    "            # Merge full-frame + zoom detections with NMS\n",
    "            dets = nms_xyxy_merge(dets_full + dets_zoom_all, iou_thr=ROI_MERGE_IOU)\n",
    "\n",
    "        t1 = time.perf_counter()\n",
    "        infer_ms = (t1 - t0) * 1000.0\n",
    "\n",
    "        # 4) optional GT scoring from sidecar annotations\n",
    "        tp = fp = fn = None\n",
    "        if sidecar_gt:\n",
    "            gt = sidecar_gt.get(this_frame_idx, [])\n",
    "            # Convert dets to evaluator format: (cls, conf, x1,y1,x2,y2)\n",
    "            preds_eval = [(int(d[5]), float(d[4]), float(d[0]), float(d[1]), float(d[2]), float(d[3])) for d in dets]\n",
    "            tp, fp, fn, _ = match_detections_to_gt(preds_eval, gt, iou_thr=IOU_MATCH)\n",
    "            tot_tp += tp\n",
    "            tot_fp += fp\n",
    "            tot_fn += fn\n",
    "            tot_frames_scored += 1\n",
    "\n",
    "        # 5) overlay + display\n",
    "        vis = draw_dets(frame, dets, class_names=class_names)\n",
    "\n",
    "        # FPS\n",
    "        t_hist.append(t1)\n",
    "        fps_inst = 0.0\n",
    "        if len(t_hist) >= 2:\n",
    "            fps_inst = (len(t_hist) - 1) / max(1e-9, (t_hist[-1] - t_hist[0]))\n",
    "\n",
    "        # UI throttle\n",
    "        now = time.perf_counter()\n",
    "        if DISPLAY_MODE == 'window':\n",
    "            if DISPLAY_MAX_FPS <= 0:\n",
    "                show = True\n",
    "            else:\n",
    "                show = (now - last_ui_t) >= (1.0 / float(DISPLAY_MAX_FPS))\n",
    "            if show:\n",
    "                last_ui_t = now\n",
    "                cv2.imshow(f\"{method_name} | ROI={ROI_MODE} | FPS~{fps_inst:.1f}\", vis)\n",
    "                # ESC to exit\n",
    "                if cv2.waitKey(1) & 0xFF == 27:\n",
    "                    break\n",
    "        elif DISPLAY_MODE == 'inline':\n",
    "            if DISPLAY_MAX_FPS <= 0:\n",
    "                show = True\n",
    "            else:\n",
    "                show = (now - last_ui_t) >= (1.0 / float(DISPLAY_MAX_FPS))\n",
    "            if show:\n",
    "                last_ui_t = now\n",
    "                _inline_show(vis)\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.write(vis)\n",
    "\n",
    "        if (processed % LOG_EVERY_N_FRAMES) == 0:\n",
    "            frame_logs.append({\n",
    "                'frame_idx': this_frame_idx,\n",
    "                'method': method_name,\n",
    "                'roi_mode': ROI_MODE,\n",
    "                'roi_triggered': int(do_roi),\n",
    "                'num_det': len(dets),\n",
    "                'infer_ms': infer_ms,\n",
    "                'fps_inst': fps_inst,\n",
    "                'tp': tp,\n",
    "                'fp': fp,\n",
    "                'fn': fn,\n",
    "            })\n",
    "\n",
    "        processed += 1\n",
    "\n",
    "    cap.release()\n",
    "    if writer is not None:\n",
    "        writer.release()\n",
    "    if DISPLAY_MODE == 'window':\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    # Summary\n",
    "    precision = recall = f1 = None\n",
    "    if tot_frames_scored > 0:\n",
    "        precision = tot_tp / (tot_tp + tot_fp + 1e-9)\n",
    "        recall = tot_tp / (tot_tp + tot_fn + 1e-9)\n",
    "        f1 = 2*precision*recall / (precision + recall + 1e-9)\n",
    "\n",
    "    df_frames = pd.DataFrame(frame_logs)\n",
    "    df_frames.to_csv(out_frame_csv, index=False)\n",
    "\n",
    "    summary = {\n",
    "        'video': str(VIDEO_PATH),\n",
    "        'method': method_name,\n",
    "        'roi_mode': ROI_MODE,\n",
    "        'frames_processed': int(processed),\n",
    "        'frames_scored': int(tot_frames_scored),\n",
    "        'precision@0.5': precision,\n",
    "        'recall@0.5': recall,\n",
    "        'f1@0.5': f1,\n",
    "        'out_video': str(out_video_path) if save_video else '',\n",
    "        'frame_log_csv': str(out_frame_csv),\n",
    "    }\n",
    "\n",
    "    pd.DataFrame([summary]).to_csv(out_summary_csv, index=False)\n",
    "    print(\"Saved:\", out_frame_csv)\n",
    "    print(\"Saved:\", out_summary_csv)\n",
    "    if save_video:\n",
    "        print(\"Saved:\", out_video_path)\n",
    "\n",
    "    return summary\n"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "969a175b",
   "metadata": {},
   "source": [
    "## Run one method (interactive benchmark)\n",
    "\n",
    "Set `METHOD` and `USE_ROI_ZOOM` and run.\n",
    "\n",
    "Tips:\n",
    "\n",
    "- Start with `max_frames=600` for fast iteration.\n",
    "- Use `start_frame` to jump into a busy segment.\n",
    "- ROI-zoom is slower; benchmark it separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "08fd7297",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T13:07:28.793599Z",
     "start_time": "2026-01-16T13:06:59.022821Z"
    }
   },
   "source": [
    "# Cell 9 — Run a single method\n",
    "\n",
    "# Choose preprocessing method\n",
    "METHOD = \"baseline\"  # choose from PIPELINES\n",
    "\n",
    "# ROI_MODE controls crop-based zoom:\n",
    "#   'off'         => no extra inference (fastest)\n",
    "#   'conditional' => only triggers ROI when full-frame looks weak (recommended for speed)\n",
    "#   'always'      => always does extra crop inference (slowest)\n",
    "ROI_MODE = \"off\"  # edit here per run\n",
    "\n",
    "summary = run_video(\n",
    "    method_name=METHOD,\n",
    "    max_frames=600,     # set None to run full video\n",
    "    start_frame=0,\n",
    "    save_video=True,\n",
    ")\n",
    "\n",
    "pd.DataFrame([summary])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: video_benchmark_outputs\\GOPR5844_002__baseline__roioff__20260116_140659__frame_log.csv\n",
      "Saved: video_benchmark_outputs\\GOPR5844_002__baseline__roioff__20260116_140659__summary.csv\n",
      "Saved: video_benchmark_outputs\\GOPR5844_002__baseline__roioff__20260116_140659.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                               video    method roi_mode  \\\n",
       "0  S:\\IntelliJ\\Projects\\ES_Drone_Detection\\video_...  baseline      off   \n",
       "\n",
       "   frames_processed  frames_scored  precision@0.5  recall@0.5    f1@0.5  \\\n",
       "0               497            497       0.940199    0.634529  0.757697   \n",
       "\n",
       "                                           out_video  \\\n",
       "0  video_benchmark_outputs\\GOPR5844_002__baseline...   \n",
       "\n",
       "                                       frame_log_csv  \n",
       "0  video_benchmark_outputs\\GOPR5844_002__baseline...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video</th>\n",
       "      <th>method</th>\n",
       "      <th>roi_mode</th>\n",
       "      <th>frames_processed</th>\n",
       "      <th>frames_scored</th>\n",
       "      <th>precision@0.5</th>\n",
       "      <th>recall@0.5</th>\n",
       "      <th>f1@0.5</th>\n",
       "      <th>out_video</th>\n",
       "      <th>frame_log_csv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S:\\IntelliJ\\Projects\\ES_Drone_Detection\\video_...</td>\n",
       "      <td>baseline</td>\n",
       "      <td>off</td>\n",
       "      <td>497</td>\n",
       "      <td>497</td>\n",
       "      <td>0.940199</td>\n",
       "      <td>0.634529</td>\n",
       "      <td>0.757697</td>\n",
       "      <td>video_benchmark_outputs\\GOPR5844_002__baseline...</td>\n",
       "      <td>video_benchmark_outputs\\GOPR5844_002__baseline...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "82c92823",
   "metadata": {},
   "source": [
    "## Batch run (all methods)\n",
    "\n",
    "Runs all pipelines (baseline + preprocess variants) for a short segment and produces a summary table.\n",
    "\n",
    "Recommended: keep `max_frames` small (for example, 300–1000) while exploring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0c0f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 — Batch run all methods (short segment)\n",
    "\n",
    "ROI_MODE = \"off\"  # keep fast for batch scans\n",
    "\n",
    "summaries = []\n",
    "for m in PIPELINES:\n",
    "    print(\"Running:\", m)\n",
    "    s = run_video(method_name=m, max_frames=300, start_frame=0, save_video=False)\n",
    "    summaries.append(s)\n",
    "\n",
    "df = pd.DataFrame(summaries)\n",
    "df.sort_values(by=['f1@0.5','recall@0.5','precision@0.5'], ascending=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
